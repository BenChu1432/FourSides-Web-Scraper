# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# import requests
# from bs4 import BeautifulSoup
# import time

# # âœ… ç”¨ Selenium æŠ“æ–‡ç« ç¶²å€
# def get_tfc_article_urls(max_pages=1):
#     options = Options()
#     options.add_argument("--headless")
#     options.add_argument("--disable-gpu")
#     options.add_argument("--lang=zh-TW")

#     driver = webdriver.Chrome(options=options)
#     article_urls = []

#     # for page in range(1, max_pages + 1):
#     #     if page == 1:
#     #         url = "https://tfc-taiwan.org.tw/fact-check-reports-all/"
#     #     else:
#     #         url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"

#     #     print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
#     #     driver.get(url)



#     for page in range(1):
#         if page == 1:
#             url = "https://tfc-taiwan.org.tw/fact-check-reports-all/"
#         else:
#             url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"

#         print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
#         driver.get(url)
#         try:
#             WebDriverWait(driver, 10).until(
#                 EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.kb-query-item"))
#             )

#             articles = driver.find_elements(By.CSS_SELECTOR, "li.kb-query-item")
#             print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

#             for article in articles:
#                 try:
#                     a_tag = article.find_element(By.TAG_NAME, "a")
#                     href = a_tag.get_attribute("href")
#                     if href and href not in article_urls:
#                         article_urls.append(href)
#                 except Exception as e:
#                     print("âš ï¸ æ‰¾ <a> éŒ¯èª¤ï¼š", e)

#         except Exception as e:
#             print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: {e}")

#         time.sleep(1.2)

#     driver.quit()
#     print(f"\nğŸ“¦ å…±è’é›† {len(article_urls)} ç­†æ–‡ç« ç¶²å€")
#     return article_urls

# # âœ… ç”¨ BeautifulSoup æŠ“æŸ¥æ ¸è¨˜è€…ï¼ˆä½œè€…ï¼‰
# def get_author_from_article(url):
#     try:
#         res = requests.get(url)
#         res.encoding = "utf-8"
#         soup = BeautifulSoup(res.text, "html.parser")

#         for p in soup.find_all("p"):
#             strong = p.find("strong")
#             if strong and "æŸ¥æ ¸è¨˜è€…" in strong.get_text():
#                 author_text = p.get_text().replace("æŸ¥æ ¸è¨˜è€…ï¼š", "").strip()
#                 return author_text
#         return None
#     except Exception as e:
#         print(f"âŒ è®€å–æ–‡ç« å¤±æ•—: {url}ï¼ŒéŒ¯èª¤ï¼š{e}")
#         return None

# # ğŸ”„ ä¸²æ¥æµç¨‹ï¼šæŠ“ç¶²å€ â†’ ä¸€ç¯‡ç¯‡æŠ“ä½œè€…
# urls = get_tfc_article_urls(max_pages=3)

# def get_title_and_summary(url):
#     try:
#         res = requests.get(url)
#         res.encoding = "utf-8"
#         soup = BeautifulSoup(res.text, "html.parser")

#         # ç›´æ¥æŠ“æ‰€æœ‰ <strong> æ¨™ç±¤
#         strong_tags = soup.find_all("strong")

#         # ä¸ä½¿ç”¨ä»»ä½•æ–‡å­—æ¢ä»¶åˆ¤æ–·ï¼Œç›´æ¥å–å‰å…©å€‹
#         title = strong_tags[0].get_text(strip=True) if len(strong_tags) > 0 else None
#         summary = strong_tags[1].get_text(strip=True) if len(strong_tags) > 1 else None

#         return title, summary

#     except Exception as e:
#         print(f"âŒ æŠ“å–å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")
#         return None, None

# def get_article_content(url):
#     try:
#         res = requests.get(url)
#         res.encoding = "utf-8"
#         soup = BeautifulSoup(res.text, "html.parser")

#         # æŠ“æ‰€æœ‰æ®µè½
#         all_paragraphs = soup.find_all("p")

#         # éæ¿¾æ‰ç©ºçš„ã€éå…§æ–‡æ®µè½ï¼ˆå¯é€²ä¸€æ­¥åŠ æ¢ä»¶ï¼‰
#         content_paragraphs = [
#             p.get_text(strip=True)
#             for p in all_paragraphs
#             if p.get_text(strip=True) and len(p.get_text(strip=True)) > 20
#         ]

#         content = "\n".join(content_paragraphs)
#         return content if content else None

#     except Exception as e:
#         print(f"âŒ æŠ“æ–‡ç« å…§å®¹å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")
#         return None


# # def get_publish_time(soup):
# #     try:
# #         # æ‰¾å‡ºæ‰€æœ‰ <p> æ¨™ç±¤
# #         p_tags = soup.find_all("p")

# #         for p in p_tags:
# #             strong = p.find("strong")
# #             if strong and "ç™¼å¸ƒ" in strong.get_text():
# #                 # å›å‚³ strong å¾Œé¢çš„ç´”æ–‡å­—ï¼ˆä¹Ÿå°±æ˜¯æ—¥æœŸï¼‰
# # #                 return p.get_text(strip=True).replace(strong.get_text(strip=True), "").strip()

# # #         return None
# # #     except Exception as e:
# # #         print(f"âŒ ç™¼å¸ƒæ™‚é–“æ“·å–å¤±æ•—ï¼š{e}")
# # #         return None

# # print("\nğŸ“– æ¯ç¯‡æ–‡ç« æ‘˜è¦ï¼š")
# # for url in urls:
# #     res = requests.get(url)
# #     res.encoding = "utf-8"
# #     soup = BeautifulSoup(res.text, "html.parser")

# #     title, summary = get_title_and_summary(soup)
# #     article = get_article_content(soup)
# #     publish_date = get_publish_time(soup)
# #     author = get_author_from_article(soup)

# #     print(f"\nğŸ“° {url}")
# #     print(f"ğŸ“Œ æ¨™é¡Œï¼š{title if title else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
# #     print(f"ğŸ“Œ çµè«–ï¼š{summary if summary else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
# #     print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{publish_date if publish_date else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
# #     print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{author if author else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")

# #     if article:
# #         print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{article}")
# #     else:
# #         print("ğŸ“„ æ­£æ–‡å…§å®¹ï¼šï¼ˆæœªæ‰¾åˆ°ï¼‰")

# # # res = requests.get(urls)
# # # res.encoding = "utf-8"
# # # soup = BeautifulSoup(res.text, "html.parser")

# # # author = get_author_from_article(soup)
# # # title, summary = get_title_and_summary(soup)
# # # article = get_article_content(soup)
# # # publish_date = get_publish_time(soup)

# # # print(f"\nğŸ“° {urls}")
# # # print(f"ğŸ“Œ æ¨™é¡Œï¼š{title}")
# # # print(f"ğŸ“Œ çµè«–ï¼š{summary}")
# # # print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{publish_date if publish_date else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
# # # print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{author}")
# # # print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{article if article else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")


# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import requests
# import time
# import re

# # âœ… Selenium æŠ“ TFC æŸ¥æ ¸æ–‡ç« ç¶²å€
# def get_tfc_article_urls(max_pages=1):
#     options = Options()
#     options.add_argument("--headless")
#     options.add_argument("--disable-gpu")
#     options.add_argument("--lang=zh-TW")
    
#     driver = webdriver.Chrome(options=options)
#     article_urls = []

#     for page in range(1, max_pages + 1):
#         url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"
#         print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
#         driver.get(url)

#         try:
#             WebDriverWait(driver, 10).until(
#                 EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.kb-query-item"))
#             )
#             articles = driver.find_elements(By.CSS_SELECTOR, "li.kb-query-item")
#             print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

#             for article in articles:
#                 try:
#                     a_tag = article.find_element(By.TAG_NAME, "a")
#                     href = a_tag.get_attribute("href")
#                     if href and href not in article_urls:
#                         article_urls.append(href)
#                 except Exception as e:
#                     print("âš ï¸ æ‰¾ <a> éŒ¯èª¤ï¼š", e)

#         except Exception as e:
#             print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: {e}")

#         time.sleep(1.2)

#     driver.quit()
#     print(f"\nğŸ“¦ å…±è’é›† {len(article_urls)} ç­†æ–‡ç« ç¶²å€")
#     return article_urls

# # âœ… æ“·å–æ¨™é¡Œèˆ‡çµè«–
# def get_title_and_summary(soup):
#     try:
#         title_tag = soup.find("h1")
#         title = title_tag.get_text(strip=True) if title_tag else None

#         conclusion = None
#         for tag in soup.find_all(["p", "div", "span"]):
#             text = tag.get_text(strip=True)
#             if text in ["éŒ¯èª¤", "éƒ¨åˆ†éŒ¯èª¤", "äº‹å¯¦é‡æ¸…", "æ­£ç¢º"]:
#                 conclusion = text
#                 break

#         return title, conclusion
#     except Exception as e:
#         print(f"âŒ æ¨™é¡Œèˆ‡çµè«–æ“·å–éŒ¯èª¤ï¼š{e}")
#         return None, None

# # âœ… æ“·å–ç™¼ä½ˆæ—¥æœŸ
# def get_publish_time(soup):
#     try:
#         for p in soup.find_all("p"):
#             strong = p.find("strong")
#             if strong and "ç™¼ä½ˆ" in strong.text:
#                 contents = p.contents
#                 for part in contents:
#                     if isinstance(part, str) and part.strip():
#                         return part.strip()
#         return None
#     except Exception as e:
#         print(f"âŒ ç™¼å¸ƒæ™‚é–“æ“·å–å¤±æ•—ï¼š{e}")
#         return None

# # âœ… æ“·å–æŸ¥æ ¸è¨˜è€…
# def get_author_from_article(soup):
#     try:
#         text = soup.get_text()
#         match = re.search(r"æŸ¥æ ¸è¨˜è€…[:ï¼š]?\s*([^\sï¼Œã€\n]+)", text)
#         if match:
#             return match.group(1)
#         return None
#     except Exception as e:
#         print(f"âŒ è¨˜è€…æ“·å–å¤±æ•—ï¼š{e}")
#         return None

# # âœ… æ“·å–æ–‡ç« å…§å®¹
# def get_article_content(soup):
#     try:
#         content = ""
#         article_div = soup.find("div", class_=lambda c: c and ("entry-content" in c or "wp-block" in c))
#         if article_div:
#             paragraphs = article_div.find_all(["p", "li"])
#             content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
#         return content if content else None
#     except Exception as e:
#         print(f"âŒ æ­£æ–‡å…§å®¹æ“·å–éŒ¯èª¤ï¼š{e}")
#         return None

# # âœ… ä¸»ç¨‹å¼ï¼šå°å‡ºæ¯ç¯‡æ–‡ç« è³‡è¨Š
# def main():
#     urls = get_tfc_article_urls(max_pages=2)

#     print("\nğŸ“– æ¯ç¯‡æ–‡ç« æ‘˜è¦ï¼š")
#     for url in urls:
#         try:
#             res = requests.get(url)
#             res.encoding = "utf-8"
#             soup = BeautifulSoup(res.text, "html.parser")

#             title, summary = get_title_and_summary(soup)
#             article = get_article_content(soup)
#             publish_date = get_publish_time(soup)
#             author = get_author_from_article(soup)

#             print(f"\nğŸ“° {url}")
#             print(f"ğŸ“Œ æ¨™é¡Œï¼š{title if title else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
#             print(f"ğŸ“Œ çµè«–ï¼š{summary if summary else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
#             print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{publish_date if publish_date else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
#             print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{author if author else 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")

#             if article:
#                 print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{article[:500]}...")  # å¯èª¿æ•´é¡¯ç¤ºé•·åº¦
#             else:
#                 print("ğŸ“„ æ­£æ–‡å…§å®¹ï¼šï¼ˆæœªæ‰¾åˆ°ï¼‰")

#         except Exception as e:
#             print(f"âŒ è®€å–å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")

# if __name__ == "__main__":
#     main()



# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import requests
# import time
# import re

# class TFCNews:
#     def __init__(self, url=None):
#         self.url = url
#         self.media_name = "TFC"
#         self.max_pages = 2
#         self.title = None
#         self.summary = None
#         self.published_at = None
#         self.authors = []
#         self.content = None
#         self.images = []
#         self.origin = "å°ç£äº‹å¯¦æŸ¥æ ¸ä¸­å¿ƒ"

#     def get_chrome_driver(self):
#         options = Options()
#         options.add_argument("--headless")
#         options.add_argument("--disable-gpu")
#         options.add_argument("--lang=zh-TW")
#         return webdriver.Chrome(options=options)

#     def _get_article_urls(self):
#         driver = self.get_chrome_driver()
#         article_urls = []

#         for page in range(1, self.max_pages + 1):
#             url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"
#             print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
#             driver.get(url)

#             try:
#                 WebDriverWait(driver, 10).until(
#                     EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.kb-query-item"))
#                 )
#                 articles = driver.find_elements(By.CSS_SELECTOR, "li.kb-query-item")
#                 print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

#                 for article in articles:
#                     try:
#                         a_tag = article.find_element(By.TAG_NAME, "a")
#                         href = a_tag.get_attribute("href")
#                         if href and href not in article_urls:
#                             article_urls.append(href)
#                     except Exception as e:
#                         print("âš ï¸ æ‰¾ <a> éŒ¯èª¤ï¼š", e)

#             except Exception as e:
#                 print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: {e}")

#             time.sleep(1.2)

#         driver.quit()
#         print(f"\nğŸ“¦ å…±è’é›† {len(article_urls)} ç­†æ–‡ç« ç¶²å€")
#         return article_urls

#     def parse_article(self, soup):
#         # æ¨™é¡Œ
#         title_tag = soup.find("h1")
#         self.title = title_tag.get_text(strip=True) if title_tag else "Missing Title"

#         # çµè«–ï¼ˆsummaryï¼‰
#         self.summary = None
#         for tag in soup.find_all(["p", "div", "span"]):
#             text = tag.get_text(strip=True)
#             if text in ["éŒ¯èª¤", "éƒ¨åˆ†éŒ¯èª¤", "äº‹å¯¦é‡æ¸…", "æ­£ç¢º"]:
#                 self.summary = text
#                 break
#         if not self.summary:
#             self.summary = "Missing Summary"

#         # ç™¼å¸ƒæ—¥æœŸ
#         self.published_at = None
#         for p in soup.find_all("p"):
#             strong = p.find("strong")
#             if strong and "ç™¼ä½ˆ" in strong.text:
#                 contents = p.contents
#                 for part in contents:
#                     if isinstance(part, str) and part.strip():
#                         self.published_at = part.strip()
#                         break
#             if self.published_at:
#                 break
#         if not self.published_at:
#             self.published_at = "Missing Date"

#         # æŸ¥æ ¸è¨˜è€…
#         self.authors = []
#         text = soup.get_text()
#         match = re.search(r"æŸ¥æ ¸è¨˜è€…[:ï¼š]?\s*([^\sï¼Œã€\n]+)", text)
#         if match:
#             self.authors.append(match.group(1))
#         else:
#             self.authors.append("Missing Author")

#         # æ­£æ–‡å…§å®¹
#         content_div = soup.find("div", class_=lambda c: c and ("entry-content" in c or "wp-block" in c))
#         if content_div:
#             paragraphs = content_div.find_all(["p", "li"])
#             self.content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
#         else:
#             self.content = "Missing Content"

#         # åœ–ç‰‡ï¼ˆå¯æ“´å……ï¼‰
#         self.images = []
#         for img in soup.find_all("img"):
#             image_url = None

#             if img.has_attr("src"):
#                 image_url = img["src"]
#             elif img.has_attr("srcset"):
#                 # å– srcset ä¸­è§£æåº¦æœ€å¤§çš„é‚£å€‹
#                 srcset = img["srcset"]
#                 candidates = [s.strip().split(" ")[0] for s in srcset.split(",")]
#                 if candidates:
#                     image_url = candidates[0]

#             if image_url:
#                 # è‹¥æ˜¯ç›¸å°è·¯å¾‘ï¼Œè£œä¸Šä¾†æºç¶²å€
#                 if self.url and image_url.startswith("/"):
#                     image_url = urljoin(self.url, image_url)
#                 self.images.append(image_url)

#     def run(self):
#         urls = self._get_article_urls()

#         print("\nğŸ“– æ¯ç¯‡æ–‡ç« æ‘˜è¦ï¼š")
#         for url in urls:
#             try:
#                 res = requests.get(url)
#                 res.encoding = "utf-8"
#                 soup = BeautifulSoup(res.text, "html.parser")

#                 self.parse_article(soup)

#                 print(f"\nğŸ“° {url}")
#                 print(f"ğŸ“Œ æ¨™é¡Œï¼š{self.title}")
#                 print(f"ğŸ“Œ çµè«–ï¼š{self.summary}")
#                 print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{self.published_at}")
#                 print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{', '.join(self.authors)}")
#                 print(f"ğŸ–¼ï¸ åœ–ç‰‡æ•¸é‡ï¼š{len(self.images)}")
#                 print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{self.content[:500]}...")

#             except Exception as e:
#                 print(f"âŒ è®€å–å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")


# import time
# import re
# import requests
# from bs4 import BeautifulSoup
# from urllib.parse import urljoin

# from urllib.parse import urljoin
# from selenium.webdriver.chrome.options import Options
# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from selenium.webdriver.chrome.options import Options


# class TFCNews:
#     def __init__(self, url=None):
#         self.url = url
#         self.media_name = "TFC"
#         self.max_pages = 2
#         self.title = None
#         self.summary = None
#         self.published_at = None
#         self.authors = []
#         self.content = None
#         self.images = []
#         self.origin = "å°ç£äº‹å¯¦æŸ¥æ ¸ä¸­å¿ƒ"

#     def get_chrome_driver(self):
#         options = Options()
#         options.add_argument("--headless")
#         options.add_argument("--disable-gpu")
#         options.add_argument("--lang=zh-TW")
#         return webdriver.Chrome(options=options)

#     def _get_article_urls(self):
#         driver = self.get_chrome_driver()
#         article_urls = []

#         for page in range(1, self.max_pages + 1):
#             url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"
#             print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
#             driver.get(url)

#             try:
#                 WebDriverWait(driver, 10).until(
#                     EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.kb-query-item"))
#                 )
#                 articles = driver.find_elements(By.CSS_SELECTOR, "li.kb-query-item")
#                 print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

#                 for article in articles:
#                     try:
#                         a_tag = article.find_element(By.TAG_NAME, "a")
#                         href = a_tag.get_attribute("href")
#                         if href and href not in article_urls:
#                             article_urls.append(href)
#                     except Exception as e:
#                         print("âš ï¸ æ‰¾ <a> éŒ¯èª¤ï¼š", e)

#             except Exception as e:
#                 print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: {e}")

#             time.sleep(1.2)

#         driver.quit()
#         print(f"\nğŸ“¦ å…±è’é›† {len(article_urls)} ç­†æ–‡ç« ç¶²å€")
#         return article_urls

#     def parse_article(self, soup):
#         # æ¨™é¡Œ
#         title_tag = soup.find("h1")
#         self.title = title_tag.get_text(strip=True) if title_tag else "Missing Title"

#         # çµè«–ï¼ˆsummaryï¼‰
#         self.summary = None
#         for tag in soup.find_all(["p", "div", "span"]):
#             text = tag.get_text(strip=True)
#             if text in ["éŒ¯èª¤", "éƒ¨åˆ†éŒ¯èª¤", "äº‹å¯¦é‡æ¸…", "æ­£ç¢º"]:
#                 self.summary = text
#                 break
#         if not self.summary:
#             self.summary = "Missing Summary"

#         # ç™¼å¸ƒæ—¥æœŸ
#         self.published_at = None
#         for p in soup.find_all("p"):
#             strong = p.find("strong")
#             if strong and "ç™¼ä½ˆ" in strong.text:
#                 contents = p.contents
#                 for part in contents:
#                     if isinstance(part, str) and part.strip():
#                         self.published_at = part.strip()
#                         break
#             if self.published_at:
#                 break
#         if not self.published_at:
#             self.published_at = "Missing Date"

#         # æŸ¥æ ¸è¨˜è€…
#         self.authors = []
#         text = soup.get_text()
#         match = re.search(r"æŸ¥æ ¸è¨˜è€…[:ï¼š]?\s*([^\sï¼Œã€\n]+)", text)
#         if match:
#             self.authors.append(match.group(1))
#         else:
#             self.authors.append("Missing Author")

#         # æ­£æ–‡å…§å®¹
#         content_div = soup.find("div", class_=lambda c: c and ("entry-content" in c or "wp-block" in c))
#         if content_div:
#             paragraphs = content_div.find_all(["p", "li"])
#             self.content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
#         else:
#             self.content = "Missing Content"


#         # åœ–ç‰‡æ“·å–ï¼ˆå¼·åŒ–ç‰ˆï¼‰
#         # âœ… åœ–ç‰‡æ“·å–ï¼ˆå¼·åŒ–ç‰ˆï¼‰
#         self.images = []
#         images = []

#         for img in soup.find_all("img"):
#             image_url = img.get("src")
            
#             if not image_url and img.get("srcset"):
#                 # å¾ srcset é¸æœ€å¤§è§£æåº¦
#                 candidates = [s.strip().split(" ")[0] for s in img["srcset"].split(",")]
#                 if candidates:
#                     image_url = candidates[-1]

#             if image_url:
#                 full_url = urljoin(url, image_url)
#                 images.append(full_url)

#         def run(self):
#             urls = self._get_article_urls()
#             driver = self.get_chrome_driver()

#         print("\nğŸ“– æ¯ç¯‡æ–‡ç« æ‘˜è¦ï¼š")
#         for url in urls:
#             try:
#                 driver.get(url)
#                 WebDriverWait(driver, 10).until(
#                     EC.presence_of_element_located((By.TAG_NAME, "article"))
#                 )
#                 soup = BeautifulSoup(driver.page_source, "html.parser")
#                 self.url = url  # è¦è¨˜å¾—è¨­ self.url æ‰èƒ½åš urljoin

#                 self.parse_article(soup)

#                 print(f"\nğŸ“° {url}")
#                 print(f"ğŸ“Œ æ¨™é¡Œï¼š{self.title}")
#                 print(f"ğŸ“Œ çµè«–ï¼š{self.summary}")
#                 print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{self.published_at}")
#                 print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{', '.join(self.authors)}")
#                 print(f"ğŸ–¼ï¸ åœ–ç‰‡æ•¸é‡ï¼š{len(self.images)}")
#                 for i, img_url in enumerate(self.images[:3]):
#                     print(f"   ğŸ“· åœ–ç‰‡{i+1}: {img_url}")
#                 print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{self.content[:300]}...")

#             except Exception as e:
#                 print(f"âŒ è®€å–å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")

#         driver.quit()


#     if __name__ == "__main__":
#         checker = TFCNews()
#         checker.run()



# import time
# import re
# from urllib.parse import urljoin
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC

# class TFCNews:
#     def __init__(self, url=None):
#         self.url = url
#         self.media_name = "TFC"
#         self.max_pages = 2
#         self.title = None
#         self.summary = None
#         self.published_at = None
#         self.authors = []
#         self.content = None
#         self.images = []
#         self.origin = "å°ç£äº‹å¯¦æŸ¥æ ¸ä¸­å¿ƒ"

#     def get_chrome_driver(self):
#         options = Options()
#         options.add_argument("--headless")
#         options.add_argument("--disable-gpu")
#         options.add_argument("--lang=zh-TW")
#         return webdriver.Chrome(options=options)

#     def _get_article_urls(self):
#         driver = self.get_chrome_driver()
#         article_urls = []

#         for page in range(1, self.max_pages + 1):
#             url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"
#             print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
#             driver.get(url)

#             try:
#                 WebDriverWait(driver, 10).until(
#                     EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.kb-query-item"))
#                 )
#                 articles = driver.find_elements(By.CSS_SELECTOR, "li.kb-query-item")
#                 print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

#                 for article in articles:
#                     try:
#                         a_tag = article.find_element(By.TAG_NAME, "a")
#                         href = a_tag.get_attribute("href")
#                         if href and href not in article_urls:
#                             article_urls.append(href)
#                     except Exception as e:
#                         print("âš ï¸ æ‰¾ <a> éŒ¯èª¤ï¼š", e)

#             except Exception as e:
#                 print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: {e}")

#             time.sleep(1.2)

#         driver.quit()
#         print(f"\nğŸ“¦ å…±è’é›† {len(article_urls)} ç­†æ–‡ç« ç¶²å€")
#         return article_urls

#     def parse_article(self, soup):
#         # æ¨™é¡Œ
#         title_tag = soup.find("h1")
#         self.title = title_tag.get_text(strip=True) if title_tag else "Missing Title"

#         # çµè«–ï¼ˆsummaryï¼‰
#         self.summary = None
#         for tag in soup.find_all(["p", "div", "span"]):
#             text = tag.get_text(strip=True)
#             if text in ["éŒ¯èª¤", "éƒ¨åˆ†éŒ¯èª¤", "äº‹å¯¦é‡æ¸…", "æ­£ç¢º"]:
#                 self.summary = text
#                 break
#         if not self.summary:
#             self.summary = "Missing Summary"

#         # ç™¼å¸ƒæ—¥æœŸ
#         self.published_at = None
#         for p in soup.find_all("p"):
#             strong = p.find("strong")
#             if strong and "ç™¼ä½ˆ" in strong.text:
#                 contents = p.contents
#                 for part in contents:
#                     if isinstance(part, str) and part.strip():
#                         self.published_at = part.strip()
#                         break
#             if self.published_at:
#                 break
#         if not self.published_at:
#             self.published_at = "Missing Date"

#         # æŸ¥æ ¸è¨˜è€…
#         self.authors = []
#         text = soup.get_text()
#         match = re.search(r"æŸ¥æ ¸è¨˜è€…[:ï¼š]?\s*([^\sï¼Œã€\n]+)", text)
#         if match:
#             self.authors.append(match.group(1))
#         else:
#             self.authors.append("Missing Author")

#         # æ­£æ–‡å…§å®¹
#         content_div = soup.find("div", class_=lambda c: c and ("entry-content" in c or "wp-block" in c))
#         if content_div:
#             paragraphs = content_div.find_all(["p", "li"])
#             self.content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
#         else:
#             self.content = "Missing Content"

#         # åœ–ç‰‡æ“·å–
#         self.images = []
#         for img in soup.find_all("img"):
#             image_url = img.get("src")
#             if not image_url and img.get("srcset"):
#                 candidates = [s.strip().split(" ")[0] for s in img["srcset"].split(",")]
#                 if candidates:
#                     image_url = candidates[-1]
#             if image_url:
#                 full_url = urljoin(self.url, image_url)
#                 self.images.append(full_url)

#     def run(self):
#         urls = self._get_article_urls()
#         driver = self.get_chrome_driver()

#         print("\nğŸ“– æ¯ç¯‡æ–‡ç« æ‘˜è¦ï¼š")
#         for url in urls:
#             try:
#                 driver.get(url)
#                 WebDriverWait(driver, 10).until(
#                     EC.presence_of_element_located((By.TAG_NAME, "article"))
#                 )
#                 soup = BeautifulSoup(driver.page_source, "html.parser")
#                 self.url = url  # è¨˜å¾—è¨­å®š self.url çµ¦åœ–ç‰‡ç”¨

#                 self.parse_article(soup)

#                 print(f"\nğŸ“° {url}")
#                 print(f"ğŸ“Œ æ¨™é¡Œï¼š{self.title}")
#                 print(f"ğŸ“Œ çµè«–ï¼š{self.summary}")
#                 print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{self.published_at}")
#                 print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{', '.join(self.authors)}")
#                 print(f"ğŸ–¼ï¸ åœ–ç‰‡æ•¸é‡ï¼š{len(self.images)}")
#                 for i, img_url in enumerate(self.images[:3]):
#                     print(f"   ğŸ“· åœ–ç‰‡{i+1}: {img_url}")
#                 print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{self.content[:300]}...")

#             except Exception as e:
#                 print(f"âŒ è®€å–å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")

#         driver.quit()

# # âœ… ä¸»ç¨‹å¼åŸ·è¡Œ
# if __name__ == "__main__":
#     checker = TFCNews()
#     checker.run()


# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# from urllib.parse import urljoin

# import requests




# # TFC article URL
# url = "https://tfc-taiwan.org.tw/fact-check-reports/fake-one-page-ad-dr-chiang-kun-chun-insomnia-medicine/"

# # Get HTML
# res = requests.get(url)
# soup = BeautifulSoup(res.text, "html.parser")

# # Find all .jpg images
# jpg_images = []
# for img in soup.find_all("img"):
#     src = img.get("src")
#     if src and src.endswith(".jpg"):
#         full_url = urljoin(url, src)
#         jpg_images.append(full_url)

# # Print result
# print(f"ğŸ–¼ï¸ Found {len(jpg_images)} JPG image(s):")
# for i, img_url in enumerate(jpg_images, 1):
#     print(f"ğŸ“· Image {i}: {img_url}")
# # âœ… æ–‡ç« ç¶²å€
# url = "https://tfc-taiwan.org.tw/fact-check-reports/fake-one-page-ad-dr-chiang-kun-chun-insomnia-medicine/"

# # âœ… å»ºç«‹ Headless Chrome
# options = Options()
# #options.add_argument("--headless")
# options.add_argument("--disable-gpu")
# options.add_argument("--lang=zh-TW")
# driver = webdriver.Chrome(options=options)

# # âœ… é–‹å•Ÿç¶²é ä¸¦ç­‰å¾…æ–‡ç« è¼‰å…¥
# driver.get(url)
# WebDriverWait(driver, 20).until(
#     EC.presence_of_element_located((By.CSS_SELECTOR, "div.entry-content"))
# )

# # âœ… ä½¿ç”¨ BeautifulSoup è§£ææ¸²æŸ“å¾Œçš„ HTML
# soup = BeautifulSoup(driver.page_source, "html.parser")
# driver.quit()

# # âœ… æ“·å–åœ–ç‰‡
# image_urls = []
# for img in soup.find_all("img"):
#     image_url = img.get("src")

#     # è‹¥ src ç‚ºç©ºï¼Œè©¦è‘—å¾ srcset æŠ“æœ€å¤§è§£æåº¦
#     if not image_url and img.get("srcset"):
#         candidates = [s.strip().split(" ")[0] for s in img["srcset"].split(",")]
#         if candidates:
#             image_url = candidates[-1]

#     if image_url:
#         full_url = urljoin(url, image_url)
#         image_urls.append(full_url)

# # âœ… é¡¯ç¤ºåœ–ç‰‡çµæœ
# print(driver.page_source[:500])  # å°å‡ºå‰ 500 å­—å…ƒçš„ HTMLï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯ç©ºç™½é 
# print(f"ğŸ–¼ï¸ æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡ï¼š")
# for i, img_url in enumerate(image_urls, 1):
#     print(f"ğŸ“· åœ–ç‰‡{i}: {img_url}")


import time
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import requests

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class TFCNews:
    def __init__(self, url=None):
        self.url = url
        self.media_name = "TFC"
        self.max_pages = 2
        self.title = None
        self.summary = None
        self.published_at = None
        self.authors = []
        self.content = None
        self.images = []
        self.origin = "å°ç£äº‹å¯¦æŸ¥æ ¸ä¸­å¿ƒ"

    def get_chrome_driver(self):
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--lang=zh-TW")
        return webdriver.Chrome(options=options)

    def _get_article_urls(self):
        driver = self.get_chrome_driver()
        article_urls = []

        for page in range(1, self.max_pages + 1):
            url = f"https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={page}"
            print(f"ğŸ”— é–‹å•Ÿç¬¬ {page} é : {url}")
            driver.get(url)

            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li.kb-query-item"))
                )
                articles = driver.find_elements(By.CSS_SELECTOR, "li.kb-query-item")
                print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

                for article in articles:
                    try:
                        a_tag = article.find_element(By.TAG_NAME, "a")
                        href = a_tag.get_attribute("href")
                        if href and href not in article_urls:
                            article_urls.append(href)
                    except Exception as e:
                        print("âš ï¸ æ‰¾ <a> éŒ¯èª¤ï¼š", e)

            except Exception as e:
                print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: {e}")

            time.sleep(1.2)

        driver.quit()
        print(f"\nğŸ“¦ å…±è’é›† {len(article_urls)} ç­†æ–‡ç« ç¶²å€")
        return article_urls

    def parse_article(self, soup):
        # æ¨™é¡Œï¼ˆå…ˆæ‰¾ <h1>ï¼Œæ‰¾ä¸åˆ°å† fallback åˆ° <strong>ï¼‰
        title_tag = soup.find("h1")
        if title_tag and title_tag.get_text(strip=True):
            self.title = title_tag.get_text(strip=True)
        else:
            # fallback: æ‰¾ç¬¬ä¸€å€‹ <strong> æ¨™é¡Œæ¨£å¼
            strong_tags = soup.find_all("strong")
            for tag in strong_tags:
                text = tag.get_text(strip=True)
                if text and len(text) > 10:
                    self.title = text
                    break
            else:
                self.title = "Missing Title"

        # çµè«–ï¼ˆsummaryï¼‰
        self.summary = None
        for tag in soup.find_all(["p", "div", "span"]):
            text = tag.get_text(strip=True)
            if text in ["éŒ¯èª¤", "éƒ¨åˆ†éŒ¯èª¤", "äº‹å¯¦é‡æ¸…", "æ­£ç¢º"]:
                self.summary = text
                break
        if not self.summary:
            self.summary = "Missing Summary"

        # ç™¼å¸ƒæ—¥æœŸ
        self.published_at = None
        for p in soup.find_all("p"):
            strong = p.find("strong")
            if strong and "ç™¼ä½ˆ" in strong.text:
                contents = p.contents
                for part in contents:
                    if isinstance(part, str) and part.strip():
                        self.published_at = part.strip()
                        break
            if self.published_at:
                break
        if not self.published_at:
            self.published_at = "Missing Date"

        # æŸ¥æ ¸è¨˜è€…
        self.authors = []
        text = soup.get_text()
        match = re.search(r"æŸ¥æ ¸è¨˜è€…[:ï¼š]?\s*([^\sï¼Œã€\n]+)", text)
        if match:
            self.authors.append(match.group(1))
        else:
            self.authors.append("Missing Author")

        # æ­£æ–‡å…§å®¹
        content_div = soup.find("div", class_=lambda c: c and ("entry-content" in c or "wp-block" in c))
        if content_div:
            paragraphs = content_div.find_all(["p", "li"])
            self.content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        else:
            self.content = "Missing Content"

        # åœ–ç‰‡æ“·å–ï¼ˆ.jpgï¼‰
        self.images = []
        for img in soup.find_all("img"):
            image_url = img.get("src")
            if not image_url and img.get("srcset"):
                candidates = [s.strip().split(" ")[0] for s in img["srcset"].split(",")]
                if candidates:
                    image_url = candidates[-1]
            if image_url and ".jpg" in image_url:
                full_url = urljoin(self.url, image_url)
                self.images.append(full_url)

    def run(self):
        urls = self._get_article_urls()
        driver = self.get_chrome_driver()

        print("\nğŸ“– æ¯ç¯‡æ–‡ç« æ‘˜è¦ï¼š")
        for url in urls:
            try:
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "article"))
                )
                soup = BeautifulSoup(driver.page_source, "html.parser")
                self.url = url  # çµ¦åœ–ç‰‡ç”¨

                self.parse_article(soup)

                # åˆä½µåœ–ç‰‡æ¬„ä½
                images_combined = " | ".join(self.images) if self.images else "No Images"

                print(f"\nğŸ“° {url}")
                print(f"ğŸ“Œ æ¨™é¡Œï¼š{self.title}")
                print(f"ğŸ“Œ çµè«–ï¼š{self.summary}")
                print(f"ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š{self.published_at}")
                print(f"ğŸ“ æŸ¥æ ¸è¨˜è€…ï¼š{', '.join(self.authors)}")
                print(f"ğŸ–¼ï¸ åœ–ç‰‡æ¬„ä½ï¼š{images_combined}")
                print(f"ğŸ“„ æ­£æ–‡å…§å®¹ï¼š\n{self.content[:300]}...")

            except Exception as e:
                print(f"âŒ è®€å–å¤±æ•—ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")

        driver.quit()


# âœ… ä¸»ç¨‹å¼åŸ·è¡Œ
if __name__ == "__main__":
    checker = TFCNews()
    checker.run()