import asyncio
import re
from typing import Any, Dict, List, Optional
import json
import os
from dotenv import load_dotenv
import google.generativeai as genai

# External deps in your project
from app.modals.newsEntity import NewsEntity
from scrapers.news import AssessmentItem
from util import traditionalChineseUtil

# --- Optional: Together integration stub (kept for completeness, unchanged behavior) ---
try:
    from together import Together
    _together_available = True
except Exception:
    _together_available = False

load_dotenv()

# Configure providers
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel("models/gemini-2.5-flash-lite")

# Together client (lazy init)
_TOGETHER_MODEL = os.getenv("TOGETHER_MODEL", "Qwen/Qwen3-235B-A22B-fp8-tput")
_TOGETHER_ENABLED = os.getenv("TOGETHER_API_KEY") is not None and _together_available

_together_client = None
def _get_together_client():
    global _together_client
    if _together_client is None and _TOGETHER_ENABLED:
        _together_client = Together(api_key=os.getenv("TOGETHER_API_KEY"))
    return _together_client

def get_clickbait_data_from_together(headline: str) -> Optional[dict]:
    if not _TOGETHER_ENABLED:
        return None
    client = _get_together_client()
    if client is None:
        return None

    system_prompt = """
ä½ æ˜¯ä¸€ä½æ–°èåˆ†æåŠ©ç†ï¼Œå°ˆé–€è² è²¬åˆ¤æ–·æ–°èæ–‡ç« æ¨™é¡Œæ˜¯å¦å±¬æ–¼æ¨™é¡Œé»¨ï¼Œä¸¦æä¾›ä¿¡å¿ƒåˆ†æ•¸ã€å…·é«”èªªæ˜èˆ‡ä¸­æ€§æ¨™é¡Œå»ºè­°ã€‚

åš´æ ¼è¼¸å‡ºè¦å‰‡ï¼ˆå‹™å¿…éµå®ˆï¼‰ï¼š
- åƒ…è¼¸å‡ºã€Œä¸€å€‹ã€JSON ç‰©ä»¶ï¼Œä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€èªªæ˜æˆ–ç¨‹å¼ç¢¼å€å¡Šã€‚
- ä¸è¦ä½¿ç”¨ Markdown åœæ¬„ï¼ˆä¾‹å¦‚ ```jsonï¼‰ã€‚
- åƒ…èƒ½ä½¿ç”¨é ‚å±¤éµï¼šclickbaitã€‚
- ä»»ä½•å­—ä¸²ä¸­çš„è‹±æ–‡é›™å¼•è™Ÿ " éœ€ä»¥ \\" è½‰ç¾©ï¼›å¯ä»¥ä½¿ç”¨å…¨å½¢å¼•è™Ÿã€Œã€ä¸éœ€è½‰ç¾©ã€‚
- ä¸è¦ä½¿ç”¨å–®ä¸€æ”¶å°¾å¼•è™Ÿ â€™ é€ æˆ JSON å­—ä¸²ä¸åˆæ³•ã€‚
- å­—ä¸²ä¸­çš„æ›è¡Œè«‹ä½¿ç”¨ \\nã€‚
- ä¸è¦åŒ…å«å¤šé¤˜é€—è™Ÿï¼ˆtrailing commasï¼‰ã€‚
- clickbait.confidence å¿…é ˆç‚º 0 åˆ° 1 çš„æ•¸å­—ï¼ˆå…©ä½å°æ•¸ï¼‰ï¼Œexplanation å’Œ refined_title ç‚ºéç©ºå­—ä¸²ã€‚
""".strip()

    try:
        resp = client.chat.completions.create(
            model=_TOGETHER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": headline.strip()}
            ]
        )
        raw = resp.choices[0].message.content
        data = safe_parse_json(raw)
        cb = data.get("clickbait")
        if isinstance(cb, dict):
            conf = _coerce_float_0_1(cb.get("confidence"))
            exp = cb.get("explanation") if isinstance(cb.get("explanation"), str) and cb.get("explanation").strip() else None
            rt = cb.get("refined_title") if isinstance(cb.get("refined_title"), str) and cb.get("refined_title").strip() else None
            if conf is not None and exp and rt:
                return {"confidence": conf, "explanation": exp.strip(), "refined_title": rt.strip()}
    except Exception:
        return None
    return None

# ----------------------------------------------

ALLOWED_TAGS = {
    "journalistic_merits": [
        "**multiple_perspectives**ï¼ˆå¤šå…ƒè§€é»ï¼‰",
        "**logical_flow**ï¼ˆé‚è¼¯æ¸…æ™°ï¼‰",
        "**use_of_real_world_examples**ï¼ˆå¯¦ä¾‹å…·é«”ï¼‰",
        "**constructive_criticism**ï¼ˆå…·å»ºè¨­æ€§æ‰¹è©•ï¼‰",
        "**avoidance_of_nationalism_or_populism**ï¼ˆé¿å…æ°‘æ—ä¸»ç¾©æˆ–æ°‘ç²¹ä¸»ç¾©ï¼‰",
        "**source_transparency**ï¼ˆæ¶ˆæ¯ä¾†æºé€æ˜ï¼‰",
        "**clarification_of_uncertainty**ï¼ˆæ¾„æ¸…ä¸ç¢ºå®šæ€§ï¼‰",
        "**background_context**ï¼ˆèƒŒæ™¯è„ˆçµ¡å……åˆ†ï¼‰",
        "**transparency_of_process(å ±å°éç¨‹é€æ˜)",
        "**depth_of_analysis** (åˆ†ææ·±å…¥)",
        "**engagement_with_complexity** (æœ‰è™•ç†è­°é¡Œçš„è¤‡é›œæ€§)",
        "**local_relevance_and_contextualization** (åœ¨åœ°è„ˆçµ¡åŒ–èˆ‡é—œè¯æ€§)",
        "**independence_from_power** (å ±å°ç¨ç«‹æ€§å¼·)",
        "**clarity_of_purpose** (å ±å°ç›®çš„æ˜ç¢º)",
        "**accountability_framing**ï¼ˆæŒä»½è€…è²¬ä»»æ­¸å±¬æ˜ç¢ºï¼‰",
        "**ethical_reporting**ï¼ˆå…·å€«ç†æ„è­˜çš„å ±å°ï¼‰",
        "**use_of_data**ï¼ˆå–„ç”¨æ•¸æ“šï¼‰",
        "**cultural_humility**ï¼ˆå±•ç¾æ–‡åŒ–è¬™éœï¼‰",
        "**centering_affected_voices**ï¼ˆå‡¸é¡¯ç•¶äº‹è€…è§€é»ï¼‰",
        "**readability**ï¼ˆè¡¨é”æ˜“æ‡‚ï¼‰",
        "**headline_reflects_content**ï¼ˆæ¨™é¡Œèˆ‡å…§æ–‡ä¸€è‡´ï¼‰",
        "**public_interest_orientation**ï¼ˆä»¥å…¬å…±åˆ©ç›Šç‚ºå°å‘ï¼‰",
        "**critical_thinking_encouraged**ï¼ˆä¿ƒé€²æ‰¹åˆ¤æ€è€ƒï¼‰",
        "**timely_relevance_and_timeless_insight**ï¼ˆå ±é“å…·æ™‚æ•ˆæ€§å’Œé•·é å•Ÿç™¼æ€§ï¼‰"
    ],
    "journalistic_demerits": [
        "**decontextualisation**ï¼ˆè„«é›¢èªå¢ƒ/ç¼ºä¹ç´°ç·»è„ˆçµ¡ï¼‰",
        "**clickbait**ï¼ˆæ¨™é¡Œé»¨ï¼‰",
        "**fear_mongering**ï¼ˆæƒ¡æ„å¼•èµ·ç¤¾æœƒææ…Œï¼‰",
        "**cherry_picking**ï¼ˆé¸æ“‡æ€§èˆ‰ä¾‹ï¼‰",
        "**loaded_language**ï¼ˆæƒ…ç·’æ€§ç”¨èªï¼‰",
        "**conflation**ï¼ˆä¸ç•¶æ··æ·†ï¼‰",
        "**lack_of_balance**ï¼ˆç¼ºä¹å¹³è¡¡è§€é»ï¼‰",
        "**overemphasis_on_profanity_and_insults**ï¼ˆéåº¦æ”¾å¤§ç²—ä¿—èªè¨€æˆ–äººèº«æ”»æ“Šï¼‰",
        "**social_media_amplification_trap**ï¼ˆç¤¾ç¾¤æ”¾å¤§é™·é˜±ï¼‰",
        "**nationalistic framing**ï¼ˆæ°‘æ—ä¸»ç¾©æ¡†æ¶ï¼‰",
        "**corporate_glorification**ï¼ˆä¼æ¥­ç¾åŒ–ï¼‰",
        "**overemphasis_on_glory**ï¼ˆéåº¦å¼·èª¿æˆå°±ï¼‰",
        "**propagandistic_tone**ï¼ˆå¤§å¤–å®£èªèª¿ï¼‰",
        "**overuse_of_statistics_without_verification**ï¼ˆæ•¸æ“šæ¿«ç”¨æˆ–æœªé©—è­‰ï¼‰",
        "**no_critical_inquiry_or_accountability**ï¼ˆç¼ºä¹æ‰¹åˆ¤èˆ‡è²¬ä»»è¿½ç©¶ï¼‰",
        "**strategic_omission**ï¼ˆç­–ç•¥æ€§å¿½ç•¥ï¼‰",
        "**anonymous_authority**ï¼ˆä¸å…·åæ¬Šå¨ï¼‰",
        "**minor_incident_magnification**ï¼ˆå°äº‹ä»¶èª‡å¤§ï¼‰",
        "**victimhood_framing**ï¼ˆå—å®³è€…æ¡†æ¶ï¼‰",
        "**heroic_framing**ï¼ˆè‹±é›„æ•˜äº‹ï¼‰",
        "**binary_framing**ï¼ˆéé»‘å³ç™½æ•˜äº‹ï¼‰",
        "**moral_judgment_framing**ï¼ˆé“å¾·åˆ¤æ–·åŒ…è£ï¼‰",
        "**cultural_essentialism**ï¼ˆæ–‡åŒ–æœ¬è³ªè«–ï¼‰",
        "**traditional_values_shield**ï¼ˆä¸»å¼µå‚³çµ±åƒ¹å€¼ä½œæ“‹ç‰Œï¼‰",
        "**pre_criminal_framing**ï¼ˆé è¨­æœ‰ç½ªï¼‰"
    ],
    "reporting_style": [
        "he_said_she_said_reporting",
        "propagandistic_reporting",
        "investigative_reporting",
        "solutions_journalism",
        "feature_reporting",
        "advocacy_journalism",
        "opinion_reporting",
        "sensationalist_reporting",
        "stenographic_reporting",
        "data_journalism",
        "explanatory_reporting",
        "entertainment_reporting",
        "infotainment_reporting",
        "patriotic_reporting"
    ]
}

journalistic_merits_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["journalistic_merits"]])
misguiding_tools_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["journalistic_demerits"]])
reporting_style_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["reporting_style"]])

# ---- System prompt (hardened) ----
system_prompt = f"""
ä½ æ˜¯ä¸€ä½æ–°èåˆ†æåŠ©ç†ï¼Œå°ˆé–€è² è²¬åˆ¤æ–·æ–°èæ–‡ç« ä¸­å¤šå¤§ç¨‹åº¦ä¸Šå­˜åœ¨ä»¥ä¸‹ç‰¹å®šçš„æ–°èå„ªé»å’Œèª¤å°æ€§å ±å°æŠ€è¡“ï¼Œä¸¦é‡å°æ¯ä¸€é …æä¾›æ¸…æ¥šã€æœ‰æ ¹æ“šçš„èªªæ˜ã€‚

åš´æ ¼è¼¸å‡ºè¦å‰‡ï¼ˆå‹™å¿…éµå®ˆï¼‰ï¼š
- åƒ…è¼¸å‡ºã€Œä¸€å€‹ã€JSON ç‰©ä»¶ï¼Œä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€èªªæ˜æˆ–ç¨‹å¼ç¢¼å€å¡Šã€‚
- ä¸è¦ä½¿ç”¨ Markdown åœæ¬„ï¼ˆä¾‹å¦‚ ```jsonï¼‰ã€‚
- åƒ…èƒ½ä½¿ç”¨é ‚å±¤éµï¼šclickbaitã€journalistic_demeritsã€journalistic_meritsã€reporting_styleã€reporting_intentionã€‚
- ä»»ä½•å­—ä¸²ä¸­çš„è‹±æ–‡é›™å¼•è™Ÿ " éœ€ä»¥ \\" è½‰ç¾©ï¼›å¯ä»¥ä½¿ç”¨å…¨å½¢å¼•è™Ÿã€Œã€ä¸éœ€è½‰ç¾©ã€‚
- ä¸è¦ä½¿ç”¨å–®ä¸€æ”¶å°¾å¼•è™Ÿ â€™ é€ æˆ JSON å­—ä¸²ä¸åˆæ³•ã€‚
- å­—ä¸²ä¸­çš„æ›è¡Œè«‹ä½¿ç”¨ \\nã€‚
- ä¸è¦åŒ…å«å¤šé¤˜é€—è™Ÿï¼ˆtrailing commasï¼‰ã€‚
- clickbait.confidence å¿…é ˆç‚º 0 åˆ° 1 çš„æ•¸å­—ï¼ˆå…©ä½å°æ•¸ï¼‰ï¼Œexplanation/refined_title ç‚ºéç©ºå­—ä¸²ã€‚
- åƒ…åŒ…å«å¯¦éš›å‡ºç¾ä¸”é©ç”¨çš„æ¨™ç±¤ï¼ˆmerits/demeritsï¼‰ï¼Œæ²’æœ‰å‡ºç¾å°±çœç•¥è©²å­éµã€‚

---
1) æ–°èå ±é“æ¨™é¡Œé»¨ç¨‹åº¦ï¼ˆclickbaitï¼‰
- è©•ä¼°æ¨™é¡Œæ˜¯å¦æœ‰èª‡å¼µå½¢å®¹ã€æåš‡èªã€è³£é—œå­ã€çµ•å°åŒ–ç­‰ç‰¹å¾µã€‚
- ä¿¡å¿ƒåˆ†æ•¸ï¼š
  - 0.00--0.30ï¼šç„¡æ˜é¡¯æ¨™é¡Œé»¨å…ƒç´ 
  - 0.31--0.60ï¼šè¼•å¾®å¸ç›
  - 0.61--0.85ï¼šå¤šç¨®ç‰¹å¾µä¸”èª‡å¼µ
  - 0.86--1.00ï¼šåš´é‡èª‡å¼µæˆ–èˆ‡å…§æ–‡è½å·®å¤§
- refined_titleï¼šä¸­æ€§å…‹åˆ¶ã€ç›´æ¥åæ˜ å…§æ–‡ï¼Œä¸ç•™æ‡¸å¿µã€‚

2a) èª¤å°æ‰‹æ³•ï¼ˆjournalistic_demeritsï¼‰
åªæ¨™ç¤ºæœ‰é—œæˆ–å‡ºç¾éçš„ï¼š
{misguiding_tools_list}

2b) æ–°èå„ªé»ï¼ˆjournalistic_meritsï¼‰
åªæ¨™ç¤ºæœ‰å…·é«”é«”ç¾çš„ï¼š
{journalistic_merits_list}

3) æ–°èå ±é“é¢¨æ ¼ï¼ˆreporting_styleï¼‰
{reporting_style_list}

4) æ–°èå ±é“ç›®çš„ï¼ˆreporting_intentionï¼‰
è‡ªæ“¬ 1-3 é …ï¼Œæ¯é …æœ€å¤š 10 å­—ã€‚

è¼¸å‡º JSON ç¯„ä¾‹ï¼ˆéµåå›ºå®šï¼›åƒ…ç¤ºæ„å‹æ…‹ï¼Œå¯¦éš›åªè¼¸å‡ºæœ‰å‡ºç¾çš„å­éµï¼‰ï¼š
{{
  "clickbait": {{
    "confidence": 0.00,
    "explanation": "â€¦â€¦",
    "refined_title": "â€¦â€¦"
  }},
  "journalistic_demerits": {{
    "anonymous_authority": {{
      "description": "â€¦â€¦",
      "degree": "low"
    }}
  }},
  "journalistic_merits": {{
    "headline_reflects_content": {{
      "description": "â€¦â€¦",
      "degree": "high"
    }}
  }},
  "reporting_style": ["feature_reporting", "explanatory_reporting"],
  "reporting_intention": ["äº‹å¯¦å ±å°", "äº‹ä»¶é‡æ¸…"]
}}
"""

# ---------- Helpers ----------
def _is_retriable_error_msg(msg: str) -> bool:
    msg = (msg or "").upper()
    retriable_tokens = ("500", "503", "504", "INTERNAL", "UNAVAILABLE", "DEADLINE_EXCEEDED", "TIMEOUT")
    return any(tok in msg for tok in retriable_tokens)

def _normalize_degree(val: str) -> str:
    if not isinstance(val, str):
        return "low"
    v = val.strip().lower()
    return v if v in {"low", "moderate", "high"} else "low"

def _build_clean_allowed_set(raw_tags: List[str]) -> set:
    clean = set()
    for t in raw_tags:
        if t.startswith("**") and t.count("**") >= 2:
            core = t.split("**")[1].strip()
        else:
            core = t
        core = core.replace("ï¼ˆ", " ").replace("ï¼‰", " ").strip()
        clean.add(core)
    return clean

_CLEAN_ALLOWED_DEMERITS = _build_clean_allowed_set(ALLOWED_TAGS["journalistic_demerits"])
_CLEAN_ALLOWED_MERITS = _build_clean_allowed_set(ALLOWED_TAGS["journalistic_merits"])

def _clean_key(key: str) -> str:
    if not isinstance(key, str):
        return ""
    k = key.strip()
    if k.startswith("**") and k.count("**") >= 2:
        k = k.split("**")[1].strip()
    k = k.replace("ï¼ˆ", " ").replace("ï¼‰", " ").strip()
    return k

def _coerce_float_0_1(x: Any) -> Optional[float]:
    try:
        f = float(x)
        if f < 0:
            f = 0.0
        if f > 1:
            f = 1.0
        return round(f + 1e-8, 2)
    except Exception:
        return None

def _strip_code_fences_and_duplicates(content: str) -> str:
    s = content.strip()
    s = re.sub(r"```[a-zA-Z]*", "", s)
    s = s.replace("```", "")
    s = re.sub(r'\bjson_str\s*:\s*\{[\s\S]*?\}\s*$', '', s, flags=re.IGNORECASE)
    first_obj = _extract_first_top_level_json_object(s)
    if first_obj is not None:
        return first_obj
    return s

def _extract_first_top_level_json_object(s: str) -> Optional[str]:
    start = s.find("{")
    if start == -1:
        return None
    depth = 0
    in_str = False
    escape = False
    for i in range(start, len(s)):
        ch = s[i]
        if in_str:
            if escape:
                escape = False
            elif ch == "\\":
                escape = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return s[start:i+1]
    return None

def safe_parse_json(content: str) -> dict:
    cleaned = _strip_code_fences_and_duplicates(content)
    json_candidate = _extract_first_top_level_json_object(cleaned)
    if json_candidate is None:
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start == -1 or end == -1 or end <= start:
            raise ValueError("No JSON object found in model output")
        json_candidate = cleaned[start:end+1]
    j = json_candidate
    j = j.replace("â€™", "'").replace("â€˜", "'")
    j = j.replace("â€œ", '"').replace("â€", '"')
    j = re.sub(r",\s*([}\]])", r"\1", j)
    j = j.replace("\t", "    ")
    def _strip_ctrl(m: re.Match) -> str:
        return " "
    j = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", _strip_ctrl, j)
    return json.loads(j)

def _set_empty_fields(a: NewsEntity):
    a.refined_title = None
    a.reporting_style = []
    a.reporting_intention = []
    a.journalistic_demerits = {}
    a.journalistic_merits = {}
    a.clickbait = None

def _extract_clickbait(data: dict) -> Optional[dict]:
    cb = data.get("clickbait")
    if not isinstance(cb, dict):
        rt = data.get("refined_title")
        if isinstance(rt, str) and rt.strip():
            return {
                "confidence": None,
                "explanation": None,
                "refined_title": rt.strip()
            }
        return None
    conf = _coerce_float_0_1(cb.get("confidence"))
    exp = cb.get("explanation") if isinstance(cb.get("explanation"), str) and cb.get("explanation").strip() else None
    rt = cb.get("refined_title") if isinstance(cb.get("refined_title"), str) and cb.get("refined_title").strip() else None
    if conf is None and not exp and not rt:
        return None
    return {
        "confidence": conf,
        "explanation": exp.strip() if isinstance(exp, str) else None,
        "refined_title": rt.strip() if isinstance(rt, str) else None
    }

# C. å¼·åŒ–ï¼šå®¹å¿å­—ä¸²å½¢å¼çš„ reporting_styleï¼Œå…ˆåŒ…è£æˆ list å†éæ¿¾
def _extract_reporting_style(data: dict) -> List[str]:
    rs = data.get("reporting_style", [])
    if isinstance(rs, str) and rs.strip():
        rs = [rs.strip()]
    if not isinstance(rs, list):
        return []
    allowed = set(ALLOWED_TAGS["reporting_style"])
    return [t for t in rs if isinstance(t, str) and t in allowed]

def _extract_reporting_intention(data: dict) -> List[str]:
    ri = data.get("reporting_intention", [])
    if not isinstance(ri, list):
        return []
    out = []
    for x in ri:
        if isinstance(x, (str, int, float)):
            sx = str(x).strip()
            sx = sx.strip().strip("ã€Œã€\"'")
            if sx:
                out.append(sx[:10])
        if len(out) >= 3:
            break
    return out

def _extract_tagged_section(
    data: dict,
    key: str,
    allowed_set: set
) -> Dict[str, AssessmentItem]:
    raw = data.get(key, {})
    out: Dict[str, AssessmentItem] = {}
    if not isinstance(raw, dict):
        return out
    for k, v in raw.items():
        if not isinstance(v, dict):
            continue
        clean_key = _clean_key(k)
        if clean_key not in allowed_set:
            continue
        # ä¸å…è¨±ã€Œclickbaitã€ä½œç‚ºåŠ£å‹¢æ¨™ç±¤é€é
        if clean_key.lower() == "clickbait":
            continue
        desc = v.get("description", "")
        deg = v.get("degree", "")
        if isinstance(desc, str) and desc.strip():
            if isinstance(deg, str) and deg.strip().lower() == "not applicable":
                continue
            out[clean_key] = {
                "description": desc.strip(),
                "degree": _normalize_degree(deg)
            }
    return out

def validate_schema(d: dict) -> Optional[str]:
    if "clickbait" not in d or not isinstance(d["clickbait"], dict):
        return "missing 'clickbait' object"
    cb = d["clickbait"]
    if not isinstance(cb.get("refined_title"), str) or not cb["refined_title"].strip():
        return "missing 'clickbait.refined_title'"
    conf = _coerce_float_0_1(cb.get("confidence"))
    if conf is None:
        return "invalid 'clickbait.confidence' (must be 0..1 number)"
    if "reporting_style" in d and not isinstance(d["reporting_style"], list):
        return "'reporting_style' must be a list"
    if "reporting_intention" in d and not isinstance(d["reporting_intention"], list):
        return "'reporting_intention' must be a list"
    for k in ("journalistic_demerits", "journalistic_merits"):
        if k in d and not isinstance(d[k], dict):
            return f"'{k}' must be an object"
    return None

# --- Disable clickbait detection switch (kept) ---
DISABLE_CLICKBAIT_DETECTION = os.getenv("DISABLE_CLICKBAIT_DETECTION", "true").lower() in ("1", "true", "yes")

def _force_no_clickbait(article: NewsEntity):
    refined = getattr(article, "refined_title", None) or getattr(article, "title", None) or ""
    article.clickbait = {
        "confidence": 0.00,
        "explanation": "æœªæª¢å‡ºæ¨™é¡Œé»¨å…ƒç´ ã€‚",
        "refined_title": refined.strip() or "ï¼ˆç„¡æ¨™é¡Œï¼‰"
    }

# ---------- Main ----------
async def classify_article(article: NewsEntity, max_retries: int = 3):
    print("ğŸŒˆ classifying the news:", getattr(article, "url", None))
    _set_empty_fields(article)

    content = article.content or ""
    user_prompt = f"""è«‹åˆ†æä»¥ä¸‹æ–°èæ–‡ç« ï¼Œä¸¦ä¾ system prompt çš„æ ¼å¼èˆ‡è¦å‰‡è¼¸å‡ºçµæ§‹åŒ– JSON åˆ†æçµæœï¼š

--- ARTICLE START ---
{traditionalChineseUtil.safeTranslateIntoTraditionalChinese(content)}
--- ARTICLE END ---
"""

    delay = 0.8
    for attempt in range(max_retries):
        try:
            chat = model.start_chat(history=[{"role": "user", "parts": [system_prompt.strip()]}])
            response = chat.send_message(user_prompt.strip())
            print("âœ… Got an LLM response")

            # Parse and sanitize JSON
            try:
                data = safe_parse_json(response.text)
            except Exception as parse_err:
                msg = str(parse_err)
                if _is_retriable_error_msg(msg) and attempt < max_retries - 1:
                    await asyncio.sleep(delay)
                    delay = min(delay * 2, 6.0)
                    continue
                print("âš ï¸ Failed to parse JSON:", parse_err)
                return {"ok": False, "error": f"parse_error: {msg}"}

            if not isinstance(data, dict):
                return {"ok": False, "error": "parse_error: model output is not a JSON object"}

            # A. åœ¨ validate_schema å‰å…ˆå¯¬é¬†æ­£è¦åŒ– reporting_style
            rs_pre = data.get("reporting_style")
            if isinstance(rs_pre, str) and rs_pre.strip():
                data["reporting_style"] = [rs_pre.strip()]
            elif rs_pre is None:
                data["reporting_style"] = []

            # Strict schema validation before extraction
            schema_err = validate_schema(data)
            if schema_err:
                # Allow one retry if schema invalid and attempts remain
                if attempt < max_retries - 1:
                    await asyncio.sleep(delay)
                    delay = min(delay * 2, 6.0)
                    continue
                return {"ok": False, "error": f"parse_error: {schema_err}"}

            # Extract sections
            clickbait_obj = _extract_clickbait(data)
            reporting_style_out = _extract_reporting_style(data)  # C. å¢å¼·å¾Œçš„ extractor
            reporting_intention_out = _extract_reporting_intention(data)
            journalistic_demerits_out = _extract_tagged_section(
                data, "journalistic_demerits", _CLEAN_ALLOWED_DEMERITS
            )
            journalistic_merits_out = _extract_tagged_section(
                data, "journalistic_merits", _CLEAN_ALLOWED_MERITS
            )

            # Strict validation for clickbait JSON (since schema requires a JSONB)
            if not clickbait_obj or not isinstance(clickbait_obj, dict):
                return {"ok": False, "error": "parse_error: missing or invalid 'clickbait' object"}
            if not clickbait_obj.get("refined_title"):
                return {"ok": False, "error": "parse_error: missing clickbait.refined_title"}

            # Normalize confidence to float or None
            if "confidence" in clickbait_obj:
                clickbait_obj["confidence"] = _coerce_float_0_1(clickbait_obj["confidence"])

            # Truncate explanation if extremely long (defensive)
            if isinstance(clickbait_obj.get("explanation"), str):
                clickbait_obj["explanation"] = clickbait_obj["explanation"].strip()

            # Attach to the article
            article.clickbait = clickbait_obj
            article.refined_title = clickbait_obj.get("refined_title")
            article.reporting_style = reporting_style_out
            article.reporting_intention = reporting_intention_out
            article.journalistic_demerits = journalistic_demerits_out
            article.journalistic_merits = journalistic_merits_out

            # Optional: Together headline augmentation (kept but disabled by default)
            """
            if getattr(article, "title", None):
                together_cb = get_clickbait_data_from_together(article.title)
                if together_cb:
                    article.clickbait.update({k: v for k, v in together_cb.items() if v is not None})
                    if together_cb.get("refined_title"):
                        article.refined_title = together_cb["refined_title"]
            """

            # Enforce: Do not detect clickbait (per request)
            if DISABLE_CLICKBAIT_DETECTION:
                _force_no_clickbait(article)

            print("ğŸ¥³ Successfully attached data to the article")
            return {"ok": True}

        except Exception as e:
            msg = str(e)
            if _is_retriable_error_msg(msg) and attempt < max_retries - 1:
                await asyncio.sleep(delay)
                delay = min(delay * 2, 6.0)
                continue
            print("âš ï¸ Classification error (final):", e)
            return {"ok": False, "error": msg}

async def classify_articles(articles: List[NewsEntity]):
    tasks = [classify_article(article) for article in articles]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results