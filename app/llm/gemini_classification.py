import asyncio
import re
from typing import Dict, List, Optional
from together import Together
import json
import os
from dotenv import load_dotenv
import google.generativeai as genai
from app.modals.newsEntity import NewsEntity
from scrapers.news import AssessmentItem
import random
from util import traditionalChineseUtil

load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel("models/gemini-2.5-flash-lite") 

ALLOWED_TAGS = {
    "journalistic_merits": [
        "**multiple_perspectives**ÔºàÂ§öÂÖÉËßÄÈªûÔºâ",
        "**logical_flow**ÔºàÈÇèËºØÊ∏ÖÊô∞Ôºâ",
        "**use_of_real_world_examples**ÔºàÂØ¶‰æãÂÖ∑È´îÔºâ",
        "**constructive_criticism**ÔºàÂÖ∑Âª∫Ë®≠ÊÄßÊâπË©ïÔºâ",
        "**avoidance_of_nationalism_or_populism**ÔºàÈÅøÂÖçÊ∞ëÊóè‰∏ªÁæ©ÊàñÊ∞ëÁ≤π‰∏ªÁæ©Ôºâ",
        "**source_transparency**ÔºàÊ∂àÊÅØ‰æÜÊ∫êÈÄèÊòéÔºâ",
        "**clarification_of_uncertainty**ÔºàÊæÑÊ∏Ö‰∏çÁ¢∫ÂÆöÊÄßÔºâ",
        "**background_context**ÔºàËÉåÊôØËÑàÁµ°ÂÖÖÂàÜÔºâ",
        "**transparency_of_process(Â†±Â∞éÈÅéÁ®ãÈÄèÊòé)",
        "**depth_of_analysis** (ÂàÜÊûêÊ∑±ÂÖ•)",
        "**engagement_with_complexity** (ÊúâËôïÁêÜË≠∞È°åÁöÑË§áÈõúÊÄß)",
        "**local_relevance_and_contextualization** (Âú®Âú∞ËÑàÁµ°ÂåñËàáÈóúËÅØÊÄß)",
        "**independence_from_power** (Â†±Â∞éÁç®Á´ãÊÄßÂº∑)",
        "**clarity_of_purpose** (Â†±Â∞éÁõÆÁöÑÊòéÁ¢∫)",
        "**accountability_framing**ÔºàÊåÅ‰ªΩËÄÖË≤¨‰ªªÊ≠∏Â±¨ÊòéÁ¢∫Ôºâ",
        "**ethical_reporting**ÔºàÂÖ∑ÂÄ´ÁêÜÊÑèË≠òÁöÑÂ†±Â∞éÔºâ",
        "**use_of_data**ÔºàÂñÑÁî®Êï∏ÊìöÔºâ",
        "**cultural_humility**ÔºàÂ±ïÁèæÊñáÂåñË¨ôÈÅúÔºâ",
        "**centering_affected_voices**ÔºàÂá∏È°ØÁï∂‰∫ãËÄÖËßÄÈªûÔºâ",
        "**readability**ÔºàË°®ÈÅîÊòìÊáÇÔºâ",
        "**headline_reflects_content**ÔºàÊ®ôÈ°åËàáÂÖßÊñá‰∏ÄËá¥Ôºâ",
        "**public_interest_orientation**Ôºà‰ª•ÂÖ¨ÂÖ±Âà©ÁõäÁÇ∫Â∞éÂêëÔºâ",
        "**critical_thinking_encouraged**Ôºà‰øÉÈÄ≤ÊâπÂà§ÊÄùËÄÉÔºâ",
        "**timely_relevance_and_timeless_insight**ÔºàÂ†±ÈÅìÂÖ∑ÊôÇÊïàÊÄßÂíåÈï∑ÈÅ†ÂïüÁôºÊÄßÔºâ"
    ],
    "journalistic_demerits":[
        "**decontextualisation**ÔºàËÑ´Èõ¢Ë™ûÂ¢É/Áº∫‰πèÁ¥∞Á∑ªËÑàÁµ°Ôºâ",
        "**clickbait**ÔºàÊ®ôÈ°åÈª®Ôºâ",
        "**fear_mongering**ÔºàÊÉ°ÊÑèÂºïËµ∑Á§æÊúÉÊÅêÊÖåÔºâ",
        "**cherry_picking**ÔºàÈÅ∏ÊìáÊÄßËàâ‰æãÔºâ",
        "**loaded_language**ÔºàÊÉÖÁ∑íÊÄßÁî®Ë™ûÔºâ",
        "**conflation**Ôºà‰∏çÁï∂Ê∑∑Ê∑ÜÔºâ",
        "**lack_of_balance**ÔºàÁº∫‰πèÂπ≥Ë°°ËßÄÈªûÔºâ",
        "**overemphasis_on_profanity_and_insults**ÔºàÈÅéÂ∫¶ÊîæÂ§ßÁ≤ó‰øóË™ûË®ÄÊàñ‰∫∫Ë∫´ÊîªÊìäÔºâ",
        "**social_media_amplification_trap**ÔºàÁ§æÁæ§ÊîæÂ§ßÈô∑Èò±Ôºâ",
        "**nationalistic framing**ÔºàÊ∞ëÊóè‰∏ªÁæ©Ê°ÜÊû∂Ôºâ",
        "**corporate_glorification**Ôºà‰ºÅÊ•≠ÁæéÂåñÔºâ",
        "**overemphasis_on_glory**ÔºàÈÅéÂ∫¶Âº∑Ë™øÊàêÂ∞±Ôºâ",
        "**propagandistic_tone**ÔºàÂ§ßÂ§ñÂÆ£Ë™ûË™øÔºâ",
        "**overuse_of_statistics_without_verification**ÔºàÊï∏ÊìöÊø´Áî®ÊàñÊú™È©óË≠âÔºâ",
        "**no_critical_inquiry_or_accountability**ÔºàÁº∫‰πèÊâπÂà§ËàáË≤¨‰ªªËøΩÁ©∂Ôºâ",
        "**strategic_omission**ÔºàÁ≠ñÁï•ÊÄßÂøΩÁï•Ôºâ",
        "**anonymous_authority**Ôºà‰∏çÂÖ∑ÂêçÊ¨äÂ®ÅÔºâ",
        "**minor_incident_magnification**ÔºàÂ∞è‰∫ã‰ª∂Ë™áÂ§ßÔºâ",
        "**victimhood_framing**ÔºàÂèóÂÆ≥ËÄÖÊ°ÜÊû∂Ôºâ",
        "**heroic_framing**ÔºàËã±ÈõÑÊïò‰∫ãÔºâ",
        "**binary_framing**ÔºàÈùûÈªëÂç≥ÁôΩÊïò‰∫ãÔºâ",
        "**moral_judgment_framing**ÔºàÈÅìÂæ∑Âà§Êñ∑ÂåÖË£ùÔºâ",
        "**cultural_essentialism**ÔºàÊñáÂåñÊú¨Ë≥™Ë´ñÔºâ",
        "**traditional_values_shield**Ôºà‰∏ªÂºµÂÇ≥Áµ±ÂÉπÂÄº‰ΩúÊìãÁÆ≠ÁâåÔºâ",
        "**pre_criminal_framing**ÔºàÈ†êË®≠ÊúâÁΩ™Ôºâ"
    ],
    "reporting_style": [
        "he_said_she_said_reporting", 
        "propagandistic_reporting", 
        "investigative_reporting",
        "solutions_journalism", 
        "feature_reporting", 
        "advocacy_journalism", 
        "opinion_reporting", 
        "sensationalist_reporting", 
        "stenographic_reporting",
        "data_journalism", 
        "explanatory_reporting",
        "entertainment_reporting",
        "infotainment_reporting",
        "patriotic_reporting"
    ]
}
journalistic_merits_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["journalistic_merits"]])
misguiding_tools_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["journalistic_demerits"]])
reporting_style = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["reporting_style"]])

# political standing
system_prompt = f"""
‰Ω†ÊòØ‰∏Ä‰ΩçÊñ∞ËÅûÂàÜÊûêÂä©ÁêÜÔºåÂ∞àÈñÄË≤†Ë≤¨Âà§Êñ∑Êñ∞ËÅûÊñáÁ´†‰∏≠Â§öÂ§ßÁ®ãÂ∫¶‰∏äÂ≠òÂú®‰ª•‰∏ãÁâπÂÆöÁöÑÊñ∞ËÅûÂÑ™ÈªûÂíåË™§Â∞éÊÄßÂ†±Â∞éÊäÄË°ìÔºå‰∏¶ÈáùÂ∞çÊØè‰∏ÄÈ†ÖÊèê‰æõÊ∏ÖÊ•ö„ÄÅÊúâÊ†πÊìöÁöÑË™™Êòé„ÄÇ

---

1.Ëã•ÊñáÁ´†Ê®ôÈ°åÂåÖÂê´„ÄåÊ®ôÈ°åÈª®ÔºèËÅ≥Âãï„ÄçÁâπÂæµÔºàÂ¶ÇË™áÂºµÂΩ¢ÂÆπ„ÄÅÊÅêÂöáÊÄßÊé™Ëæ≠„ÄÅÈÅéÂ∫¶ÁµïÂ∞çÂåñ„ÄÅË≥£ÈóúÂ≠êË™ûÂè•ÔºâÔºåË´ãÂú®Ëº∏Âá∫ JSON ÁöÑÊúÄ‰∏äÂ±§Âä†ÂÖ• "refined_title" Ê¨Ñ‰ΩçÔºåÊèê‰æõ‰∏ÄÂÄãÊõ¥Ê∫ñÁ¢∫„ÄÅÂÖãÂà∂‰∏îËàáÂÖßÊñá‰∏ÄËá¥ÁöÑÊ®ôÈ°åÔºõËã•ÁÑ°Ê≠§ÂïèÈ°åÔºå"refined_title" Ë´ãÂ°´ null„ÄÇË´ã‰ª•ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´ refined_title„ÄÇ

2.Ë´ã‰æùÊìö‰ª•‰∏ãÂÖ©ÁµÑÊ®ôÁ±§ÈÄ≤Ë°åÂàÜÊûêÔºö

2a.### üìå Ë™§Â∞éÊâãÊ≥ïÔºàjournalistic demeritsÔºâ
ÈÄô‰∫õÊòØÂèØËÉΩË™§Â∞éËÆÄËÄÖÁöÑÂ†±Â∞éÊäÄË°ìÔºåÂè™Ê®ôÁ§∫ÊúâÈóúÊàñÂá∫ÁèæÈÅéÁöÑÔºö

{misguiding_tools_list}

2b.### üìå Êñ∞ËÅûÂÑ™ÈªûÔºàjournalistic meritsÔºâ
ÈÄô‰∫õÊòØËÉΩÊèêÂçáÊñ∞ËÅûÂìÅË≥™ÁöÑÁâπÂæµÔºåË´ãÂà§Êñ∑ÊòØÂê¶ÊúâÂÖ∑È´îÈ´îÁèæÔºö

{journalistic_merits_list}

3.### üìå Êñ∞ËÅûÂ†±ÈÅìÈ¢®Ê†ºÔºàreporting stylesÔºâ
{reporting_style}

4.### üìå Êñ∞ËÅûÂ†±ÈÅìÁõÆÁöÑÔºàreporting intentionÔºâ
Ëá™Áî±ÁôºÊèÆ
---

### ‚ö†Ô∏è Ë´ãÊ≥®ÊÑèÔºö
- **ÂÉÖÂàóÂá∫ÂØ¶ÈöõÂú®ÊñáÁ´†‰∏≠Âá∫ÁèæÁöÑÊ®ôÁ±§**ÔºàÁÑ°Ë´ñÊòØË™§Â∞éÂ∑•ÂÖ∑ÊàñÊñ∞ËÅûÂÉπÂÄºÁâπÂæµÔºâ„ÄÇ
- ÊØè‰∏ÄÈ†ÖÊ®ôË®ªË´ãÊèê‰æõÂÖ∑È´îÊèèËø∞ËàáË©ï‰º∞Á®ãÂ∫¶Ôºå‰∏¶ÂºïÁî®ÊñáÁ´†‰∏≠ÁöÑÂ≠óË©û„ÄÅÂè•Â≠êÊàñÊÆµËêΩ‰ΩúÁÇ∫‰æùÊìö„ÄÇ
- Âè™È°ØÁ§∫ÈÅ©Áî®ÁöÑË™§Â∞éÊâãÊ≥ïÔºàjournalistic demeritsÔºâÂíåÊñ∞ËÅûÂÑ™ÈªûÔºàjournalistic meritsÔºâ, ‰ΩÜÂøÖÈ†àÈ°ØÁ§∫"refined_title", Êñ∞ËÅûÂ†±ÈÅìÈ¢®Ê†ºÔºàreporting stylesÔºâÂíåÊñ∞ËÅûÂ†±ÈÅìÁõÆÁöÑÔºàreporting intentionÔºâ
- Ëº∏Âá∫ÁØÑ‰æãÊ†ºÂºèÂøÖÈ†àÁÇ∫Ê®ôÊ∫ñ JSONÔºåÁõ¥Êé•Ëº∏Âá∫Á¥î JSON ÁµêÊßãÔºå‰∏çÈúÄË¶ÅÈ°çÂ§ñÂåÖË£ùÂú® content Ê¨Ñ‰Ωç‰∏ã„ÄÇ

---
{{
  "refined_title": "Ëã•ÈúÄË¶Å‰øÆË®ÇÂâáÂ°´ÂÖ•‰øÆË®ÇÂæåÊ®ôÈ°åÔºõÂê¶ÂâáÁÇ∫ null",
  "journalistic_demerits": {{
    "decontextualisation": {{
      "description": "Ë´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂÖ∑È´îË©≥Á¥∞ÊèèËø∞Ë©≤Ë™§Â∞éÊäÄË°ìÂú®ÊñáÁ´†‰∏≠ÊòØÂê¶Âá∫ÁèæÔºå‰ª•ÂèäÁî®ÊñáÁ´†‰∏≠ÁöÑÂÖ∑È´îÁî®Ë©ûËß£ÈáãÂá∫ÁèæÁöÑÊñπÂºè„ÄÅÁ®ãÂ∫¶ËàáË™ûÂ¢ÉÔºå‰∏¶ÈúÄË¶ÅÊ∫ñÁ¢∫ÂºïÁî®‰∫∫„ÄÅÁâ©Âíå‰∫ãË™™Êòé„ÄÇ",
      "degree": "low / moderate / high"
    }},
    ...
  }},
  "journalistic_merits": {{
    "multiple_perspectives": {{
      "description": "Ë´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂÖ∑È´îË©≥Á¥∞ÊèèËø∞Ë©≤Êñ∞ËÅûÂÑ™ÈªûÂú®ÊñáÁ´†‰∏≠ÊòØÂê¶Âá∫ÁèæÔºå‰ª•ÂèäÁî®ÊñáÁ´†‰∏≠ÁöÑÂÖ∑È´îÁî®Ë©ûËß£ÈáãÂá∫ÁèæÁöÑÊñπÂºè„ÄÅÁ®ãÂ∫¶ËàáË™ûÂ¢ÉÔºå‰∏¶ÈúÄË¶ÅÊ∫ñÁ¢∫ÂºïÁî®‰∫∫„ÄÅÁâ©Âíå‰∫ãÊòé„ÄÇ",
      "degree": "low / moderate / high"
    }},
    ...
  }},
  "reporting_style": [ÈÅ∏Áî®ÈÅ©Áî®ÁöÑÂ†±ÈÅìÈ¢®Ê†º, ...],
  "reporting_intention": [Áî®ÊúÄÂ§ö10Â≠óÊ∫ñÁ¢∫ÊåáÂá∫1-3ÂÄãÂ†±ÈÅìÁõÆÁöÑÂíåÁî®ÊÑè, ...],
}}
"""

print("system_prompt:",system_prompt)


def safe_parse_json(content: str):
    # ÂòóË©¶Âæû markdown Ê†ºÂºè‰∏≠ÊèêÂèñÁ¥î JSON ÂçÄÂ°ä
    match = re.search(r"```json\s*([\s\S]+?)\s*```", content)
    if not match:
        # Ëã•ÁÑ° markdown Ê®ôË®òÔºåÁõ¥Êé•ÂæûÁ¨¨‰∏ÄÂÄã { ÈñãÂßã
        match = re.search(r"\{[\s\S]+", content)
        if not match:
            raise ValueError("‚ö†Ô∏è ÁÑ°Ê≥ïÊâæÂà∞ JSON ÂçÄÂ°ä")
    try:
        json_str = match.group(1) if "```" in match.group(0) else match.group(0)
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print("‚ùå JSON decode error at character:", e.pos)
        print("content:",content)
        print("json_str:",json_str)
        print("‚õî ÂïèÈ°åÈôÑËøëÂÖßÂÆπÔºö", json_str[e.pos - 30:e.pos + 30])
        raise

def _set_empty_fields(a):
    a.refined_title = None
    a.reporting_style = []
    a.reporting_intention = []
    a.journalistic_demerits = {}
    a.journalistic_merits = {}

def _is_retriable_error_msg(msg: str) -> bool:
    # Based on Gemini troubleshooting guide: 500 INTERNAL, 503 UNAVAILABLE, 504 DEADLINE_EXCEEDED
    # Also handle common wording
    msg = (msg or "").upper()
    retriable_tokens = ("500", "503", "504", "INTERNAL", "UNAVAILABLE", "DEADLINE_EXCEEDED", "TIMEOUT")
    return any(tok in msg for tok in retriable_tokens)

async def classify_article(article: NewsEntity, max_retries: int = 3):
    print("üåà classifying the news:", article.url)

    # Prefill defaults so downstream never breaks
    _set_empty_fields(article)

    # Optional: truncate to mitigate deadline/exceeded and 500 due to very long context
    content = article.content or ""

    user_prompt = f"""Ë´ãÂàÜÊûê‰ª•‰∏ãÊñ∞ËÅûÊñáÁ´†Ôºå‰∏¶‰æù system prompt ÁöÑÊ†ºÂºèËàáË¶èÂâáËº∏Âá∫ÁµêÊßãÂåñ JSON ÂàÜÊûêÁµêÊûú:

--- ARTICLE START ---
{traditionalChineseUtil.safeTranslateIntoTraditionalChinese(content)}
--- ARTICLE END ---
"""

    delay = 0.8  # backoff starting delay
    for attempt in range(max_retries):
        try:
            chat = model.start_chat(history=[{"role": "user", "parts": [system_prompt.strip()]}])
            # Set a timeout so calls don't hang forever (504 guidance: increase timeout if needed)
            response = chat.send_message(user_prompt.strip())
            print("‚úÖ Gotten an LLM response")

            try:
                data = safe_parse_json(response.text)
            except Exception as parse_err:
                # Parsing error isn't a backend 500/503/504; treat as non-retriable unless model hinted at a service issue
                msg = str(parse_err)
                if _is_retriable_error_msg(msg) and attempt < max_retries - 1:
                    await asyncio.sleep(delay + random.random() * 0.5)
                    delay *= 2
                    continue
                print("‚ö†Ô∏è Failed to parse JSON:", parse_err)
                return {"ok": False, "error": f"parse_error: {msg}"}

            if not isinstance(data, dict):
                return {"ok": False, "error": "parse_error: model output is not a JSON object"}

            # Initialize locals
            refined_title: Optional[str] = None
            reporting_style_out: List[str] = []
            reporting_intention_out: List[str] = []
            journalistic_demerits_out: Dict[str, AssessmentItem] = {}
            journalistic_merits_out: Dict[str, AssessmentItem] = {}

            def _normalize_degree(val: str) -> str:
                allowed = {"low", "moderate", "high"}
                return val.lower() if isinstance(val, str) and val.lower() in allowed else "low"

            # refined_title
            rt = data.get("refined_title")
            refined_title = rt.strip() if isinstance(rt, str) and rt.strip() else None

            # reporting_style
            rs = data.get("reporting_style", [])
            if isinstance(rs, list):
                reporting_style_out = [t for t in rs if isinstance(t, str) and t in ALLOWED_TAGS["reporting_style"]]

            # reporting_intention
            ri = data.get("reporting_intention", [])
            if isinstance(ri, list):
                reporting_intention_out = [str(x).strip() for x in ri if isinstance(x, (str, int, float))][:3]

            # journalistic_demerits
            jd = data.get("journalistic_demerits", {})
            if isinstance(jd, dict):
                clean_allowed = set()
                for t in ALLOWED_TAGS["journalistic_demerits"]:
                    clean = t.split("**")[1].strip() if t.startswith("**") and "**" in t[2:] else t
                    clean = clean.replace("Ôºà", " ").replace("Ôºâ", " ").strip().split()[0]
                    clean_allowed.add(clean)
                for key, item in jd.items():
                    clean_key = key.strip("* ").split("**")[-1] if "**" in key else key.strip()
                    clean_key = clean_key.replace("Ôºà", " ").replace("Ôºâ", " ").strip().split()[0]
                    if clean_key in clean_allowed and isinstance(item, dict):
                        desc = item.get("description", "")
                        deg = item.get("degree", "")
                        if isinstance(desc, str) and desc.strip():
                            if isinstance(deg, str) and deg.lower() == "not applicable":
                                continue
                            journalistic_demerits_out[clean_key] = {
                                "description": desc.strip(),
                                "degree": _normalize_degree(deg)
                            }

            # journalistic_merits
            jm = data.get("journalistic_merits", {})
            if isinstance(jm, dict):
                clean_allowed = set()
                for t in ALLOWED_TAGS["journalistic_merits"]:
                    clean = t.split("**")[1].strip() if t.startswith("**") and "**" in t[2:] else t
                    clean = clean.replace("Ôºà", " ").replace("Ôºâ", " ").strip().split()[0]
                    clean_allowed.add(clean)
                for key, item in jm.items():
                    clean_key = key.strip("* ").split("**")[-1] if "**" in key else key.strip()
                    clean_key = clean_key.replace("Ôºà", " ").replace("Ôºâ", " ").strip().split()[0]
                    if clean_key in clean_allowed and isinstance(item, dict):
                        desc = item.get("description", "")
                        deg = item.get("degree", "")
                        if isinstance(desc, str) and desc.strip():
                            if isinstance(deg, str) and deg.lower() == "not applicable":
                                continue
                            journalistic_merits_out[clean_key] = {
                                "description": desc.strip(),
                                "degree": _normalize_degree(deg)
                            }

            # Attach to the article
            article.refined_title = refined_title
            article.reporting_style = reporting_style_out
            article.reporting_intention = reporting_intention_out
            article.journalistic_demerits = journalistic_demerits_out
            article.journalistic_merits = journalistic_merits_out

            print("ü•≥ Successfully attached data to the article")
            return {"ok": True}

        except Exception as e:
            msg = str(e)
            if _is_retriable_error_msg(msg) and attempt < max_retries - 1:
                await asyncio.sleep(delay + random.random() * 0.5)
                delay *= 2
                continue
            # Per Gemini troubleshooting guide:
            # - 500 INTERNAL: unexpected error -> reduce input or switch model; retry already attempted
            # - 503 UNAVAILABLE: service overloaded -> retry already attempted
            # - 504 DEADLINE_EXCEEDED: increase timeout -> we used 60s; consider higher if needed
            print("‚ö†Ô∏è Classification error (final):", e)
            return {"ok": False, "error": msg}

async def classify_articles(articles: List[NewsEntity]):
    tasks = [classify_article(article) for article in articles]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results