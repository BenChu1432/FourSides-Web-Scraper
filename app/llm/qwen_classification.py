import asyncio
import re
from typing import Dict, List, Optional
from together import Together
import json
import os
from dotenv import load_dotenv
import google.generativeai as genai
from app.modals.newsEntity import NewsEntity
from scrapers.news import AssessmentItem

load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel("models/gemini-2.5-flash-lite")  # or replace with latest model name
# LLAMA:meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
# Alibaba: Qwen/Qwen2.5-7B-Instruct-Turbo

ALLOWED_TAGS = {
    "journalistic_merits": [
        "**multiple_perspectives**ÔºàÂ§öÂÖÉËßÄÈªûÔºâ",
        "**logical_flow**ÔºàÈÇèËºØÊ∏ÖÊô∞Ôºâ",
        "**use_of_real_world_examples**ÔºàÂØ¶‰æãÂÖ∑È´îÔºâ",
        "**constructive_criticism**ÔºàÂÖ∑Âª∫Ë®≠ÊÄßÊâπË©ïÔºâ",
        "**avoidance_of_nationalism_or_populism**ÔºàÈÅøÂÖçÊ∞ëÊóè‰∏ªÁæ©ÊàñÊ∞ëÁ≤π‰∏ªÁæ©Ôºâ",
        "**source_transparency**ÔºàÊ∂àÊÅØ‰æÜÊ∫êÈÄèÊòéÔºâ",
        "**clarification_of_uncertainty**ÔºàÊæÑÊ∏Ö‰∏çÁ¢∫ÂÆöÊÄßÔºâ",
        "**background_context**ÔºàËÉåÊôØËÑàÁµ°ÂÖÖÂàÜÔºâ",
        "**transparency_of_process(Â†±Â∞éÈÅéÁ®ãÈÄèÊòé)",
        "**depth_of_analysis** (ÂàÜÊûêÊ∑±ÂÖ•)",
        "**engagement_with_complexity** (ÊúâËôïÁêÜË≠∞È°åÁöÑË§áÈõúÊÄß)",
        "**local_relevance_and_contextualization** (Âú®Âú∞ËÑàÁµ°ÂåñËàáÈóúËÅØÊÄß)",
        "**independence_from_power** (Â†±Â∞éÁç®Á´ãÊÄßÂº∑)",
        "**clarity_of_purpose** (Â†±Â∞éÁõÆÁöÑÊòéÁ¢∫)",
        "**accountability_framing**ÔºàÊåÅ‰ªΩËÄÖË≤¨‰ªªÊ≠∏Â±¨ÊòéÁ¢∫Ôºâ",
        "**ethical_reporting**ÔºàÂÖ∑ÂÄ´ÁêÜÊÑèË≠òÁöÑÂ†±Â∞éÔºâ",
        "**use_of_data**ÔºàÂñÑÁî®Êï∏ÊìöÔºâ",
        "**cultural_humility**ÔºàÂ±ïÁèæÊñáÂåñË¨ôÈÅúÔºâ",
        "**centering_affected_voices**ÔºàÂá∏È°ØÁï∂‰∫ãËÄÖËßÄÈªûÔºâ",
        "**readability**ÔºàË°®ÈÅîÊòìÊáÇÔºâ",
        "**headline_reflects_content**ÔºàÊ®ôÈ°åËàáÂÖßÊñá‰∏ÄËá¥Ôºâ",
        "**public_interest_orientation**Ôºà‰ª•ÂÖ¨ÂÖ±Âà©ÁõäÁÇ∫Â∞éÂêëÔºâ",
        "**critical_thinking_encouraged**Ôºà‰øÉÈÄ≤ÊâπÂà§ÊÄùËÄÉÔºâ",
        "**timely_relevance_and_timeless_insight**ÔºàÂ†±ÈÅìÂÖ∑ÊôÇÊïàÊÄßÂíåÈï∑ÈÅ†ÂïüÁôºÊÄßÔºâ"
    ],
    "journalistic_demerits":[
        "**decontextualisation**ÔºàËÑ´Èõ¢Ë™ûÂ¢É/Áº∫‰πèÁ¥∞Á∑ªËÑàÁµ°Ôºâ",
        "**clickbait**ÔºàÊ®ôÈ°åÈª®Ôºâ",
        "**fear_mongering**ÔºàÊÉ°ÊÑèÂºïËµ∑Á§æÊúÉÊÅêÊÖåÔºâ",
        "**cherry_picking**ÔºàÈÅ∏ÊìáÊÄßËàâ‰æãÔºâ",
        "**loaded_language**ÔºàÊÉÖÁ∑íÊÄßÁî®Ë™ûÔºâ",
        "**conflation**Ôºà‰∏çÁï∂Ê∑∑Ê∑ÜÔºâ",
        "**lack_of_balance**ÔºàÁº∫‰πèÂπ≥Ë°°ËßÄÈªûÔºâ",
        "**overemphasis_on_profanity_and_insults**ÔºàÈÅéÂ∫¶ÊîæÂ§ßÁ≤ó‰øóË™ûË®ÄÊàñ‰∫∫Ë∫´ÊîªÊìäÔºâ",
        "**social_media_amplification_trap**ÔºàÁ§æÁæ§ÊîæÂ§ßÈô∑Èò±Ôºâ",
        "**nationalistic framing**ÔºàÊ∞ëÊóè‰∏ªÁæ©Ê°ÜÊû∂Ôºâ",
        "**corporate_glorification**Ôºà‰ºÅÊ•≠ÁæéÂåñÔºâ",
        "**overemphasis_on_glory**ÔºàÈÅéÂ∫¶Âº∑Ë™øÊàêÂ∞±Ôºâ",
        "**propagandistic_tone**ÔºàÂ§ßÂ§ñÂÆ£Ë™ûË™øÔºâ",
        "**overuse_of_statistics_without_verification**ÔºàÊï∏ÊìöÊø´Áî®ÊàñÊú™È©óË≠âÔºâ",
        "**no_critical_inquiry_or_accountability**ÔºàÁº∫‰πèÊâπÂà§ËàáË≤¨‰ªªËøΩÁ©∂Ôºâ",
        "**strategic_omission**ÔºàÁ≠ñÁï•ÊÄßÂøΩÁï•Ôºâ",
        "**anonymous_authority**Ôºà‰∏çÂÖ∑ÂêçÊ¨äÂ®ÅÔºâ",
        "**minor_incident_magnification**ÔºàÂ∞è‰∫ã‰ª∂Ë™áÂ§ßÔºâ",
        "**victimhood_framing**ÔºàÂèóÂÆ≥ËÄÖÊ°ÜÊû∂Ôºâ",
        "**heroic_framing**ÔºàËã±ÈõÑÊïò‰∫ãÔºâ",
        "**binary_framing**ÔºàÈùûÈªëÂç≥ÁôΩÊïò‰∫ãÔºâ",
        "**moral_judgment_framing**ÔºàÈÅìÂæ∑Âà§Êñ∑ÂåÖË£ùÔºâ",
        "**cultural_essentialism**ÔºàÊñáÂåñÊú¨Ë≥™Ë´ñÔºâ",
        "**traditional_values_shield**Ôºà‰∏ªÂºµÂÇ≥Áµ±ÂÉπÂÄº‰ΩúÊìãÁÆ≠ÁâåÔºâ",
        "**pre-criminal_framing**ÔºàÈ†êË®≠ÊúâÁΩ™Ôºâ"
    ],
    "reporting_style": [
        "he_said_she_said_reporting", 
        "propagandistic_reporting", 
        "investigative_reporting",
        "solutions_journalism", 
        "feature_reporting", 
        "advocacy_journalism", 
        "opinion_reporting", 
        "sensationalist_reporting", 
        "stenographic_reporting",
        "data_journalism", 
        "explanatory_reporting"
    ]
}
journalistic_merits_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["journalistic_merits"]])
misguiding_tools_list = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["journalistic_demerits"]])
reporting_style = "\n".join([f"- {tag}" for tag in ALLOWED_TAGS["reporting_style"]])

# political standing
system_prompt = f"""
‰Ω†ÊòØ‰∏Ä‰ΩçÊñ∞ËÅûÂàÜÊûêÂä©ÁêÜÔºåÂ∞àÈñÄË≤†Ë≤¨Âà§Êñ∑Êñ∞ËÅûÊñáÁ´†‰∏≠Â§öÂ§ßÁ®ãÂ∫¶‰∏äÂ≠òÂú®‰ª•‰∏ãÁâπÂÆöÁöÑÊñ∞ËÅûÂÑ™ÈªûÂíåË™§Â∞éÊÄßÂ†±Â∞éÊäÄË°ìÔºå‰∏¶ÈáùÂ∞çÊØè‰∏ÄÈ†ÖÊèê‰æõÊ∏ÖÊ•ö„ÄÅÊúâÊ†πÊìöÁöÑË™™Êòé„ÄÇ

---

1.Ëã•ÊñáÁ´†Ê®ôÈ°åÂåÖÂê´„ÄåÊ®ôÈ°åÈª®ÔºèËÅ≥Âãï„ÄçÁâπÂæµÔºàÂ¶ÇË™áÂºµÂΩ¢ÂÆπ„ÄÅÊÅêÂöáÊÄßÊé™Ëæ≠„ÄÅÈÅéÂ∫¶ÁµïÂ∞çÂåñ„ÄÅË≥£ÈóúÂ≠êË™ûÂè•ÔºâÔºåË´ãÂú®Ëº∏Âá∫ JSON ÁöÑÊúÄ‰∏äÂ±§Âä†ÂÖ• "refined_title" Ê¨Ñ‰ΩçÔºåÊèê‰æõ‰∏ÄÂÄãÊõ¥Ê∫ñÁ¢∫„ÄÅÂÖãÂà∂‰∏îËàáÂÖßÊñá‰∏ÄËá¥ÁöÑÊ®ôÈ°åÔºõËã•ÁÑ°Ê≠§ÂïèÈ°åÔºå"refined_title" Ë´ãÂ°´ null„ÄÇË´ã‰ª•ÁπÅÈ´î‰∏≠ÊñáÊí∞ÂØ´ refined_title„ÄÇ

2.Ë´ã‰æùÊìö‰ª•‰∏ãÂÖ©ÁµÑÊ®ôÁ±§ÈÄ≤Ë°åÂàÜÊûêÔºö

2a.### üìå Ë™§Â∞éÊâãÊ≥ïÔºàjournalistic demeritsÔºâ
ÈÄô‰∫õÊòØÂèØËÉΩË™§Â∞éËÆÄËÄÖÁöÑÂ†±Â∞éÊäÄË°ìÔºåÂè™Ê®ôÁ§∫ÊúâÈóúÊàñÂá∫ÁèæÈÅéÁöÑÔºö

{misguiding_tools_list}

2b.### üìå Êñ∞ËÅûÂÑ™ÈªûÔºàjournalistic meritsÔºâ
ÈÄô‰∫õÊòØËÉΩÊèêÂçáÊñ∞ËÅûÂìÅË≥™ÁöÑÁâπÂæµÔºåË´ãÂà§Êñ∑ÊòØÂê¶ÊúâÂÖ∑È´îÈ´îÁèæÔºö

{journalistic_merits_list}

3.### üìå Êñ∞ËÅûÂ†±ÈÅìÈ¢®Ê†ºÔºàreporting stylesÔºâ
{reporting_style}

4.### üìå Êñ∞ËÅûÂ†±ÈÅìÁõÆÁöÑÔºàreporting intentionÔºâ
Ëá™Áî±ÁôºÊèÆ
---

### ‚ö†Ô∏è Ë´ãÊ≥®ÊÑèÔºö
- **ÂÉÖÂàóÂá∫ÂØ¶ÈöõÂú®ÊñáÁ´†‰∏≠Âá∫ÁèæÁöÑÊ®ôÁ±§**ÔºàÁÑ°Ë´ñÊòØË™§Â∞éÂ∑•ÂÖ∑ÊàñÊñ∞ËÅûÂÉπÂÄºÁâπÂæµÔºâ„ÄÇ
- ÊØè‰∏ÄÈ†ÖÊ®ôË®ªË´ãÊèê‰æõÂÖ∑È´îÊèèËø∞ËàáË©ï‰º∞Á®ãÂ∫¶Ôºå‰∏¶ÂºïÁî®ÊñáÁ´†‰∏≠ÁöÑÂ≠óË©û„ÄÅÂè•Â≠êÊàñÊÆµËêΩ‰ΩúÁÇ∫‰æùÊìö„ÄÇ
- Âè™È°ØÁ§∫ÈÅ©Áî®ÁöÑË™§Â∞éÊâãÊ≥ïÔºàjournalistic demeritsÔºâÂíåÊñ∞ËÅûÂÑ™ÈªûÔºàjournalistic meritsÔºâ, ‰ΩÜÂøÖÈ†àÈ°ØÁ§∫"refined_title", Êñ∞ËÅûÂ†±ÈÅìÈ¢®Ê†ºÔºàreporting stylesÔºâÂíåÊñ∞ËÅûÂ†±ÈÅìÁõÆÁöÑÔºàreporting intentionÔºâ
- Ëº∏Âá∫ÁØÑ‰æãÊ†ºÂºèÂøÖÈ†àÁÇ∫Ê®ôÊ∫ñ JSONÔºåÁõ¥Êé•Ëº∏Âá∫Á¥î JSON ÁµêÊßãÔºå‰∏çÈúÄË¶ÅÈ°çÂ§ñÂåÖË£ùÂú® content Ê¨Ñ‰Ωç‰∏ã„ÄÇ

---
{{
  "refined_title": "Ëã•ÈúÄË¶Å‰øÆË®ÇÂâáÂ°´ÂÖ•‰øÆË®ÇÂæåÊ®ôÈ°åÔºõÂê¶ÂâáÁÇ∫ null",
  "journalistic_demerits": {{
    "decontextualisation": {{
      "description": "Ë´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂÖ∑È´îË©≥Á¥∞ÊèèËø∞Ë©≤Ë™§Â∞éÊäÄË°ìÂú®ÊñáÁ´†‰∏≠ÊòØÂê¶Âá∫ÁèæÔºå‰ª•ÂèäÁî®ÊñáÁ´†‰∏≠ÁöÑÂÖ∑È´îÁî®Ë©ûËß£ÈáãÂá∫ÁèæÁöÑÊñπÂºè„ÄÅÁ®ãÂ∫¶ËàáË™ûÂ¢ÉÔºå‰∏¶ÈúÄË¶ÅÊ∫ñÁ¢∫ÂºïÁî®‰∫∫„ÄÅÁâ©Âíå‰∫ãË™™Êòé„ÄÇ",
      "degree": "low / moderate / high"
    }},
    ...
  }},
  "journalistic_merits": {{
    "multiple_perspectives": {{
      "description": "Ë´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂÖ∑È´îË©≥Á¥∞ÊèèËø∞Ë©≤Êñ∞ËÅûÂÑ™ÈªûÂú®ÊñáÁ´†‰∏≠ÊòØÂê¶Âá∫ÁèæÔºå‰ª•ÂèäÁî®ÊñáÁ´†‰∏≠ÁöÑÂÖ∑È´îÁî®Ë©ûËß£ÈáãÂá∫ÁèæÁöÑÊñπÂºè„ÄÅÁ®ãÂ∫¶ËàáË™ûÂ¢ÉÔºå‰∏¶ÈúÄË¶ÅÊ∫ñÁ¢∫ÂºïÁî®‰∫∫„ÄÅÁâ©Âíå‰∫ãÊòé„ÄÇ",
      "degree": "low / moderate / high"
    }},
    ...
  }},
  "reporting_style": [ÈÅ∏Áî®ÈÅ©Áî®ÁöÑÂ†±ÈÅìÈ¢®Ê†º, ...],
  "reporting_intention": [Á∞°Áü≠Ê∫ñÁ¢∫ÊåáÂá∫1-3ÂÄãÂ†±ÈÅìÁõÆÁöÑÂíåÁî®ÊÑè, ...],
}}
"""

print("system_prompt:",system_prompt)


def safe_parse_json(content: str):
    # ÂòóË©¶Âæû markdown Ê†ºÂºè‰∏≠ÊèêÂèñÁ¥î JSON ÂçÄÂ°ä
    match = re.search(r"```json\s*([\s\S]+?)\s*```", content)
    if not match:
        # Ëã•ÁÑ° markdown Ê®ôË®òÔºåÁõ¥Êé•ÂæûÁ¨¨‰∏ÄÂÄã { ÈñãÂßã
        match = re.search(r"\{[\s\S]+", content)
        if not match:
            raise ValueError("‚ö†Ô∏è ÁÑ°Ê≥ïÊâæÂà∞ JSON ÂçÄÂ°ä")
    try:
        json_str = match.group(1) if "```" in match.group(0) else match.group(0)
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print("‚ùå JSON decode error at character:", e.pos)
        print("‚õî ÂïèÈ°åÈôÑËøëÂÖßÂÆπÔºö", json_str[e.pos - 30:e.pos + 30])
        raise

async def classify_article(article: NewsEntity):
    print("üåà classifying the news:",article.url)
    user_prompt = f"""Ë´ãÂàÜÊûê‰ª•‰∏ãÊñ∞ËÅûÊñáÁ´†Ôºå‰∏¶‰æù system prompt ÁöÑÊ†ºÂºèËàáË¶èÂâáËº∏Âá∫ÁµêÊßãÂåñ JSON ÂàÜÊûêÁµêÊûú:

--- ARTICLE START ---
{article.content}
--- ARTICLE END ---
"""

    chat = model.start_chat(history=[
        {"role": "user", "parts": [system_prompt.strip()]},
        ])
    response = chat.send_message(user_prompt.strip())
    print("‚úÖ Gotten an LLM response")
    try:
        content = safe_parse_json(response.text)
        print("content:",content)
        print("‚úÖ Safely parsed the text")
    except Exception as e:
        print("‚ö†Ô∏è Failed to parse the json:", e)
        return None

    # Initialize safe defaults
    refined_title: Optional[str] = None
    reporting_style_out: List[str] = []
    reporting_intention_out: List[str] = []
    journalistic_demerits_out: Dict[str, AssessmentItem] = {}
    journalistic_merits_out: Dict[str, AssessmentItem] = {}

    def _normalize_degree(val: str) -> str:
        # Accept only your Degree literal, fallback to "low" if invalid
        allowed = {"low", "moderate", "high"}
        if isinstance(val, str) and val.lower() in allowed:
            return val.lower()
        # If model outputs "not applicable", just skip the item later (we only keep appeared tags)
        return "low"

    data = content
    if not isinstance(data, dict):
        print("‚ö†Ô∏è Parsed content is not a dict. Raw output:")
        print(content)
        return None
    # refined_title
    rt = data.get("refined_title", None)
    print("rt:",rt)
    if isinstance(rt, str) and rt.strip():
        refined_title = rt.strip()
        print("‚úÖ Successfully parsed the refined_title")
    else:
        refined_title = None

    # reporting_style (only keep allowed tags and ensure list[str])
    rs = data.get("reporting_style", [])
    print("rs:",rs)
    if isinstance(rs, list):
        reporting_style_out = [
            t for t in rs
            if isinstance(t, str) and t in ALLOWED_TAGS["reporting_style"]
        ]
        print("‚úÖ Successfully parsed the reporting_style")

    # reporting_intention (ensure list[str], keep short)
    ri = data.get("reporting_intention", [])
    print("ri:",ri)
    if isinstance(ri, list):
        reporting_intention_out = [str(x).strip() for x in ri if isinstance(x, (str, int, float))]
        # Optionally limit to 3
        reporting_intention_out = reporting_intention_out[:3]
        print("‚úÖ Successfully parsed the reporting_intention")

    # journalistic_demerits: keep only tags that appear and have valid structure
    jd = data.get("journalistic_demerits", {})
    print("jd:",jd)
    if isinstance(jd, dict):
        for key, item in jd.items():
            # Map keys that may come with **bold** or localized text to canonical key
            # We accept the canonical english snake_case keys from ALLOWED_TAGS
            canonical_keys = [t.split("**")[1] if t.startswith("**") else t for t in ALLOWED_TAGS["journalistic_demerits"]]
            # Build a map of clean_key -> allowed
            clean_allowed = set()
            for t in ALLOWED_TAGS["journalistic_demerits"]:
                # t is like "**decontextualisation**Ôºà‚Ä¶Ôºâ"
                # extract between ** if present
                if t.startswith("**") and "**" in t[2:]:
                    clean = t.split("**")[1].strip()
                else:
                    clean = t
                # ensure final clean is like decontextualisation
                clean = clean.replace("Ôºà", " ").replace("Ôºâ", " ").strip()
                # take first token till space or parenthesis
                clean = clean.split()[0]
                clean_allowed.add(clean)

            clean_key = key.strip("* ").split("**")[-1] if "**" in key else key.strip()
            clean_key = clean_key.replace("Ôºà", " ").replace("Ôºâ", " ").strip().split()[0]

            if clean_key in clean_allowed and isinstance(item, dict):
                desc = item.get("description", "")
                deg = item.get("degree", "").lower() if isinstance(item.get("degree"), str) else ""
                # Skip "not applicable" or empty descriptions
                if isinstance(desc, str) and desc.strip():
                    if deg == "not applicable":
                        continue
                    journalistic_demerits_out[clean_key] = {
                        "description": desc.strip(),
                        "degree": _normalize_degree(deg)
                    }
        print("‚úÖ Successfully parsed the journalistic_demerits")

    # journalistic_merits: same normalization
    jm = data.get("journalistic_merits", {})
    print("jm:",jm)
    if isinstance(jm, dict):
        clean_allowed = set()
        for t in ALLOWED_TAGS["journalistic_merits"]:
            # similar extraction
            if t.startswith("**") and "**" in t[2:]:
                clean = t.split("**")[1].strip()
            else:
                clean = t
            clean = clean.replace("Ôºà", " ").replace("Ôºâ", " ").strip()
            clean = clean.split()[0]
            clean_allowed.add(clean)

        for key, item in jm.items():
            clean_key = key.strip("* ").split("**")[-1] if "**" in key else key.strip()
            clean_key = clean_key.replace("Ôºà", " ").replace("Ôºâ", " ").strip().split()[0]
            if clean_key in clean_allowed and isinstance(item, dict):
                desc = item.get("description", "")
                deg = item.get("degree", "").lower() if isinstance(item.get("degree"), str) else ""
                if isinstance(desc, str) and desc.strip():
                    if deg == "not applicable":
                        continue
                    journalistic_merits_out[clean_key] = {
                        "description": desc.strip(),
                        "degree": _normalize_degree(deg)
                    }
        print("‚úÖ Successfully parsed the journalistic_merits")

    # Attach to article (NewsEntity should have these attributes)
    # If your NewsEntity uses different field names, adjust here
    try:
        article.refined_title = refined_title
        article.reporting_style = reporting_style_out
        article.reporting_intention = reporting_intention_out
        article.journalistic_demerits = journalistic_demerits_out
        article.journalistic_merits = journalistic_merits_out
        print("‚úÖ Successfully attached data to the article")
    except Exception as e:
        print("‚ö†Ô∏è Failed to assign fields to article:", e)
    print("ü•≥ Successfully parsed everything!")
    return {
        "refined_title": refined_title,
        "reporting_style": reporting_style_out,
        "reporting_intention": reporting_intention_out,
        "journalistic_demerits": journalistic_demerits_out,
        "journalistic_merits": journalistic_merits_out
    }

async def classify_articles(articles: List[NewsEntity]):
    tasks = [classify_article(article) for article in articles]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results